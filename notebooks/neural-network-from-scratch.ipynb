{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0738778f",
   "metadata": {},
   "source": [
    "# From Scratch: Building a Neural Network with NumPy\n",
    "\n",
    "This notebook contains an end-to-end implementation of a simple feedforward neural network using **only NumPy** â€” no deep learning frameworks involved. It aims to demystify the inner workings of neural networks by walking through each component step-by-step, with a strong focus on **clarity, interactivity, and visualization**.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Manual forward and backward passes** (no autograd)\n",
    "- Training on toy datasets (e.g., digit recognition or synthetic classification)\n",
    "- Loss and accuracy plots updated in real-time\n",
    "- Visual explanation of gradients and weight updates\n",
    "- Interactive sliders or inputs (if supported) to adjust hyperparameters\n",
    "- Clean, well-commented code intended for learning and experimentation\n",
    "\n",
    "> *This project is inspired by university coursework, but developed independently from scratch to reinforce my understanding and extend the ideas further.*\n",
    "\n",
    "In case of any questions or comments, please contact me at ea.arseneva@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "970a1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e89cde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define abstract classes for Layer, Loss, Optimizer\n",
    "\n",
    "class Layer(ABC):\n",
    "    \n",
    "    def __init__(self, input_dim=None, output_dim=None):\n",
    "        self._input_dim = input_dim\n",
    "        self._output_dim = output_dim\n",
    "        self.input = None\n",
    "\n",
    "    @property\n",
    "    def input_dim(self):\n",
    "        return self._input_dim\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self._output_dim\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass through the layer.\n",
    "        Args:\n",
    "            x (np.ndarray): Input data. The shape should be (batch_size, input_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Output data. The shape should be (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the layer.\n",
    "        Args:\n",
    "            grad (np.ndarray): Gradient of the loss with respect to the output. \n",
    "            The shape should be (batch_size, output_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the input.\n",
    "            The shape should be (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Forward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            float: Computed loss. The shape should be a scalar.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the predictions. The shape should be (batch_size,).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class Optimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Update the parameters based on the gradients.\n",
    "        Args:\n",
    "            params (np.ndarray): Parameters to be updated. The shape should be (num_params,).\n",
    "            grads (np.ndarray): Gradients of the loss with respect to the parameters. The shape should be (num_params,).\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7162cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Layer: Linear, ReLU, SoftMax, and Sequential\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape == self.input.shape, f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of ReLU is 1 for positive inputs, 0 for negative inputs\n",
    "        return grad * (self.input > 0)\n",
    "    \n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        assert input_dim > 0 and output_dim > 0, \"Input and output dimensions of a Linear layer must be positive integers.\"\n",
    "        super().__init__(input_dim, output_dim)\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None   \n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.weights.shape[0], f\"Input shape {x.shape} does not match expected shape (batch_size, {self.weights.shape[0]})\"\n",
    "        assert self.weights.shape[1] == self.bias.shape[1], f\"Weights shape {self.weights.shape} does not match bias shape {self.bias.shape}\"\n",
    "        self.input = x\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape[1] == self.bias.shape[1], f\"Gradient shape {grad.shape} does not match bias shape {self.bias.shape}\"\n",
    "        assert grad.shape[0] == self.input.shape[0], f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of the loss with respect to the input\n",
    "        grad_input = grad @ self.weights.T\n",
    "        # Gradient of the loss with respect to the weights and bias\n",
    "        self.grad_weights = self.input.T @ grad\n",
    "        self.grad_bias = np.sum(grad, axis=0, keepdims=True)\n",
    "        return grad_input\n",
    "    \n",
    "class SoftMax(Layer): #won't be used for now due to numerical stability and gradient computation issues. \n",
    "    #Instead, we will use CrossEntropyLoss\n",
    "    # and Softmax will be used in the loss function.\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.softmax(x)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape == self.output.shape, f\"Gradient shape {grad.shape} does not match output shape {self.output.shape}\"\n",
    "        # Gradient of the loss with respect to the input\n",
    "        return grad * self.output * (1 - self.output)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x: np.ndarray) -> np.ndarray:\n",
    "        # Subtract max for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        super().__init__(layers[0].input_dim, layers[-1].output_dim)\n",
    "        self.__check_consistency__()\n",
    "\n",
    "    def __check_consistency__(self):\n",
    "        assert len(self.layers) > 1, \"Sequential model must have at least one layer.\"\n",
    "        assert self.layers[0].input_dim is not None, \"First layer input dimension must be specified.\"\n",
    "        # here we assume the last layer is a  softmax layer, so we check the second last layer\n",
    "        assert self.layers[-1].output_dim is not None, \"Last layer output dimension must be specified.\"\n",
    "        assert self.layers[0].input_dim == self.input_dim, f\"First layer input dimension {self.layers[0].input_dim} does not match expected input dimension {self.input_dim}\"\n",
    "        assert self.layers[-1].output_dim == self.output_dim, f\"Last layer output dimension {self.layers[-1].output_dim} does not match expected output dimension {self.output_dim}\"\n",
    "        current_dim = self.input_dim\n",
    "        mismatch_list = []\n",
    "        for layer in self.layers:\n",
    "            if layer.input_dim != None:\n",
    "                if layer.input_dim != current_dim: \n",
    "                    mismatch_list.append(f\"Layer {layer.__class__.__name__} input dimension {layer.input_dim} does not match expected input dimension {current_dim}\")\n",
    "                current_dim = layer.output_dim\n",
    "       # if current_dim != self.layers[-2].output_dim: \n",
    "       #     mismatch_list.append(f\"Last layer output dimension {self.layers[-2].output_dim} does not match expected output dimension {current_dim}\")\n",
    "        assert len(mismatch_list) == 0, f\"Layer dimension mismatch: {'\\n'.join(mismatch_list)}\"\n",
    "            \n",
    "             \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.layers[0].input_dim, f\"Input shape {x.shape} does not match expected shape (batch_size, {self.layers[0].input_dim})\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fb14196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Loss: MeanSquaredError and CrossEntropy\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "    \n",
    "    \n",
    "class CrossEntropySoftMax(Loss):\n",
    "    # This loss function is used for multi-class classification problems.\n",
    "    # It combines softmax activation and cross-entropy loss in one function.\n",
    "    def forward(self, y_true: np.ndarray, y_pred_logits: np.ndarray) -> float:\n",
    "        assert y_true.shape == y_pred_logits.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        #apply softmax to the predictions\n",
    "        # Subtract max for numerical stability\n",
    "        self.y_pred = SoftMax.softmax(y_pred_logits)\n",
    "        #return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        loss = -np.sum(y_true * np.log(self.y_pred + 1e-15)) / y_true.shape[0]\n",
    "        return loss\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        # Clip predictions to prevent division by zero\n",
    "        return (self.y_pred - y_true)/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2cca352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Optimizer: SGD\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.01):\n",
    "        assert learning_rate > 0, \"Learning rate must be a positive number.\"\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        assert params.shape == grads.shape, f\"Parameters shape {params.shape} does not match gradients shape {grads.shape}\"\n",
    "        params -= self.learning_rate * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "ed26f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything together in a training loop\n",
    "def train(model: Sequential, loss_fn: Loss, optimizer: Optimizer, x_train: np.ndarray, y_train: np.ndarray, batch_size: int = 32, epochs: int = 1000):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        # go in batches\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model.forward(x_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            batch_loss = loss_fn.forward(y_batch, y_pred)\n",
    "            epoch_loss += batch_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            grad = loss_fn.backward(y_batch, y_pred)\n",
    "            model.backward(grad)\n",
    "            \n",
    "            # Update parameters\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, Linear):\n",
    "                    optimizer.step(layer.weights, layer.grad_weights)\n",
    "                    optimizer.step(layer.bias, layer.grad_bias)\n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= (x_train.shape[0] // batch_size)\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        #if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ba90a693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.1014933959328868\n",
      "Epoch 1, Loss: 1.1014398546187885\n",
      "Epoch 2, Loss: 1.1013927387569784\n",
      "Epoch 3, Loss: 1.1013504160186602\n",
      "Epoch 4, Loss: 1.1013126819320276\n",
      "Epoch 5, Loss: 1.10127804350833\n",
      "Epoch 6, Loss: 1.1012449527405417\n",
      "Epoch 7, Loss: 1.1012129476466834\n",
      "Epoch 8, Loss: 1.1011810512491094\n",
      "Epoch 9, Loss: 1.1011488719189635\n",
      "Epoch 10, Loss: 1.101114493348635\n",
      "Epoch 11, Loss: 1.101078452947968\n",
      "Epoch 12, Loss: 1.1010389169154529\n",
      "Epoch 13, Loss: 1.100996909147496\n",
      "Epoch 14, Loss: 1.1009517820231316\n",
      "Epoch 15, Loss: 1.1009039901854438\n",
      "Epoch 16, Loss: 1.100852682929081\n",
      "Epoch 17, Loss: 1.1007975444606948\n",
      "Epoch 18, Loss: 1.1007384051812947\n",
      "Epoch 19, Loss: 1.1006750099852904\n",
      "Epoch 20, Loss: 1.100606360525604\n",
      "Epoch 21, Loss: 1.1005327179548674\n",
      "Epoch 22, Loss: 1.1004536413644905\n",
      "Epoch 23, Loss: 1.1003688094116886\n",
      "Epoch 24, Loss: 1.1002779159944684\n",
      "Epoch 25, Loss: 1.1001808772025394\n",
      "Epoch 26, Loss: 1.1000776411558546\n",
      "Epoch 27, Loss: 1.099967547926076\n",
      "Epoch 28, Loss: 1.0998493398850375\n",
      "Epoch 29, Loss: 1.099723679701548\n",
      "Epoch 30, Loss: 1.0995881541514723\n",
      "Epoch 31, Loss: 1.09944385245691\n",
      "Epoch 32, Loss: 1.0992894246695077\n",
      "Epoch 33, Loss: 1.0991246102281833\n",
      "Epoch 34, Loss: 1.0989484334345423\n",
      "Epoch 35, Loss: 1.098760275350921\n",
      "Epoch 36, Loss: 1.0985582025494487\n",
      "Epoch 37, Loss: 1.0983411460071701\n",
      "Epoch 38, Loss: 1.0981070928357546\n",
      "Epoch 39, Loss: 1.0978580316731772\n",
      "Epoch 40, Loss: 1.0975917527886188\n",
      "Epoch 41, Loss: 1.0973091086663556\n",
      "Epoch 42, Loss: 1.097009339896508\n",
      "Epoch 43, Loss: 1.0966903841764786\n",
      "Epoch 44, Loss: 1.0963520399032134\n",
      "Epoch 45, Loss: 1.0959912083392387\n",
      "Epoch 46, Loss: 1.0956075460867867\n",
      "Epoch 47, Loss: 1.0952007432748911\n",
      "Epoch 48, Loss: 1.0947669769802693\n",
      "Epoch 49, Loss: 1.0943067290900146\n",
      "Epoch 50, Loss: 1.0938167135436734\n",
      "Epoch 51, Loss: 1.0932948446157862\n",
      "Epoch 52, Loss: 1.0927406081370916\n",
      "Epoch 53, Loss: 1.09215031678436\n",
      "Epoch 54, Loss: 1.0915225912124342\n",
      "Epoch 55, Loss: 1.0908546615776975\n",
      "Epoch 56, Loss: 1.0901457247477853\n",
      "Epoch 57, Loss: 1.0893932112201328\n",
      "Epoch 58, Loss: 1.0885932823412383\n",
      "Epoch 59, Loss: 1.0877468171734186\n",
      "Epoch 60, Loss: 1.0868458052525587\n",
      "Epoch 61, Loss: 1.0858926721009508\n",
      "Epoch 62, Loss: 1.0848814537281617\n",
      "Epoch 63, Loss: 1.0838112647960396\n",
      "Epoch 64, Loss: 1.0826768952023378\n",
      "Epoch 65, Loss: 1.081475276533563\n",
      "Epoch 66, Loss: 1.0802039495473443\n",
      "Epoch 67, Loss: 1.0788589826575448\n",
      "Epoch 68, Loss: 1.0774376369371226\n",
      "Epoch 69, Loss: 1.0759336883365742\n",
      "Epoch 70, Loss: 1.0743452175065078\n",
      "Epoch 71, Loss: 1.072671054371521\n",
      "Epoch 72, Loss: 1.0709009580004882\n",
      "Epoch 73, Loss: 1.069039928165691\n",
      "Epoch 74, Loss: 1.0670811478442124\n",
      "Epoch 75, Loss: 1.0650198322702953\n",
      "Epoch 76, Loss: 1.0628569802770558\n",
      "Epoch 77, Loss: 1.0605859013041108\n",
      "Epoch 78, Loss: 1.0582119168802877\n",
      "Epoch 79, Loss: 1.0557214161948232\n",
      "Epoch 80, Loss: 1.0531159402849057\n",
      "Epoch 81, Loss: 1.0503943695402085\n",
      "Epoch 82, Loss: 1.0475555541379975\n",
      "Epoch 83, Loss: 1.0445949096020433\n",
      "Epoch 84, Loss: 1.0415257635174247\n",
      "Epoch 85, Loss: 1.0383280702018978\n",
      "Epoch 86, Loss: 1.0350112915019964\n",
      "Epoch 87, Loss: 1.0315758740343304\n",
      "Epoch 88, Loss: 1.0280229426379697\n",
      "Epoch 89, Loss: 1.0243477511849968\n",
      "Epoch 90, Loss: 1.0205619197247893\n",
      "Epoch 91, Loss: 1.0166625507684117\n",
      "Epoch 92, Loss: 1.0126581278126936\n",
      "Epoch 93, Loss: 1.0085532005594509\n",
      "Epoch 94, Loss: 1.0043372540451334\n",
      "Epoch 95, Loss: 1.0000328700016274\n",
      "Epoch 96, Loss: 0.9956350897427928\n",
      "Epoch 97, Loss: 0.9911531588078927\n",
      "Epoch 98, Loss: 0.9866022571042136\n",
      "Epoch 99, Loss: 0.9819626327544398\n",
      "Epoch 100, Loss: 0.9772866341431864\n",
      "Epoch 101, Loss: 0.972543975784098\n",
      "Epoch 102, Loss: 0.9677336694197697\n",
      "Epoch 103, Loss: 0.9628974213610246\n",
      "Epoch 104, Loss: 0.9580216095223943\n",
      "Epoch 105, Loss: 0.9531112624126779\n",
      "Epoch 106, Loss: 0.9481800934360499\n",
      "Epoch 107, Loss: 0.9432350505379948\n",
      "Epoch 108, Loss: 0.9382862040222236\n",
      "Epoch 109, Loss: 0.9333286916227334\n",
      "Epoch 110, Loss: 0.9283763453617079\n",
      "Epoch 111, Loss: 0.9234306762578873\n",
      "Epoch 112, Loss: 0.9185034850843815\n",
      "Epoch 113, Loss: 0.9136150900291777\n",
      "Epoch 114, Loss: 0.9087500440332033\n",
      "Epoch 115, Loss: 0.9039038181396416\n",
      "Epoch 116, Loss: 0.8990931125266116\n",
      "Epoch 117, Loss: 0.8943399783879263\n",
      "Epoch 118, Loss: 0.8896149994207889\n",
      "Epoch 119, Loss: 0.8849382746811263\n",
      "Epoch 120, Loss: 0.880319225612786\n",
      "Epoch 121, Loss: 0.875755536498163\n",
      "Epoch 122, Loss: 0.8712330610940495\n",
      "Epoch 123, Loss: 0.8667649499697058\n",
      "Epoch 124, Loss: 0.8623778563655645\n",
      "Epoch 125, Loss: 0.8580586517640981\n",
      "Epoch 126, Loss: 0.853802360040063\n",
      "Epoch 127, Loss: 0.8496162547984174\n",
      "Epoch 128, Loss: 0.8455115212561994\n",
      "Epoch 129, Loss: 0.8414876352773653\n",
      "Epoch 130, Loss: 0.8375198805264703\n",
      "Epoch 131, Loss: 0.8336490849717204\n",
      "Epoch 132, Loss: 0.829846957625001\n",
      "Epoch 133, Loss: 0.8261468329507234\n",
      "Epoch 134, Loss: 0.822527799895372\n",
      "Epoch 135, Loss: 0.8189848223110139\n",
      "Epoch 136, Loss: 0.8155382164333609\n",
      "Epoch 137, Loss: 0.8121661671359611\n",
      "Epoch 138, Loss: 0.8089071477580901\n",
      "Epoch 139, Loss: 0.805727017843896\n",
      "Epoch 140, Loss: 0.8026343096074168\n",
      "Epoch 141, Loss: 0.7996268547508775\n",
      "Epoch 142, Loss: 0.7967134100678179\n",
      "Epoch 143, Loss: 0.7938905654075522\n",
      "Epoch 144, Loss: 0.7911774573583035\n",
      "Epoch 145, Loss: 0.7885380498561924\n",
      "Epoch 146, Loss: 0.7860011098153168\n",
      "Epoch 147, Loss: 0.78352657016706\n",
      "Epoch 148, Loss: 0.7811371886014203\n",
      "Epoch 149, Loss: 0.7788295561465989\n",
      "Epoch 150, Loss: 0.7766121519367138\n",
      "Epoch 151, Loss: 0.7744721224162249\n",
      "Epoch 152, Loss: 0.7724169425865036\n",
      "Epoch 153, Loss: 0.7704419862944901\n",
      "Epoch 154, Loss: 0.7685096688669517\n",
      "Epoch 155, Loss: 0.7666692365353971\n",
      "Epoch 156, Loss: 0.764918528560678\n",
      "Epoch 157, Loss: 0.7631987227352897\n",
      "Epoch 158, Loss: 0.7615621165388949\n",
      "Epoch 159, Loss: 0.7599864817039151\n",
      "Epoch 160, Loss: 0.7585199278063689\n",
      "Epoch 161, Loss: 0.7570312070818878\n",
      "Epoch 162, Loss: 0.7556607435457185\n",
      "Epoch 163, Loss: 0.7543370113383644\n",
      "Epoch 164, Loss: 0.7530380636199477\n",
      "Epoch 165, Loss: 0.7518132800126257\n",
      "Epoch 166, Loss: 0.7506469828815889\n",
      "Epoch 167, Loss: 0.7495004369822769\n",
      "Epoch 168, Loss: 0.7483970970337696\n",
      "Epoch 169, Loss: 0.7473604065600491\n",
      "Epoch 170, Loss: 0.7463536716205666\n",
      "Epoch 171, Loss: 0.7453989526044268\n",
      "Epoch 172, Loss: 0.7444430952737329\n",
      "Epoch 173, Loss: 0.7435496496314745\n",
      "Epoch 174, Loss: 0.7427034304938372\n",
      "Epoch 175, Loss: 0.7418712915703453\n",
      "Epoch 176, Loss: 0.7410820138891301\n",
      "Epoch 177, Loss: 0.7403285886290676\n",
      "Epoch 178, Loss: 0.7396072706922286\n",
      "Epoch 179, Loss: 0.7388834401346828\n",
      "Epoch 180, Loss: 0.7382109059389975\n",
      "Epoch 181, Loss: 0.7375504649684516\n",
      "Epoch 182, Loss: 0.7369220511972372\n",
      "Epoch 183, Loss: 0.7363499120552217\n",
      "Epoch 184, Loss: 0.735755923680578\n",
      "Epoch 185, Loss: 0.7352082025107457\n",
      "Epoch 186, Loss: 0.734663179063207\n",
      "Epoch 187, Loss: 0.7341567146461917\n",
      "Epoch 188, Loss: 0.7336431821163703\n",
      "Epoch 189, Loss: 0.7331552634230614\n",
      "Epoch 190, Loss: 0.7326751839396951\n",
      "Epoch 191, Loss: 0.7322130518986045\n",
      "Epoch 192, Loss: 0.7317669233451707\n",
      "Epoch 193, Loss: 0.7313438930081965\n",
      "Epoch 194, Loss: 0.7309265105427057\n",
      "Epoch 195, Loss: 0.7305184662152782\n",
      "Epoch 196, Loss: 0.7301491450423395\n",
      "Epoch 197, Loss: 0.7297634255893507\n",
      "Epoch 198, Loss: 0.7294098885423761\n",
      "Epoch 199, Loss: 0.7290575304770406\n",
      "Epoch 200, Loss: 0.7287130811819663\n",
      "Epoch 201, Loss: 0.7283702602588329\n",
      "Epoch 202, Loss: 0.7280371875092383\n",
      "Epoch 203, Loss: 0.7277122922444619\n",
      "Epoch 204, Loss: 0.7273965055366004\n",
      "Epoch 205, Loss: 0.7270940829261953\n",
      "Epoch 206, Loss: 0.7267968747039734\n",
      "Epoch 207, Loss: 0.7265293063490658\n",
      "Epoch 208, Loss: 0.7262301850404103\n",
      "Epoch 209, Loss: 0.7259577602587186\n",
      "Epoch 210, Loss: 0.7256975137429343\n",
      "Epoch 211, Loss: 0.7254290423165944\n",
      "Epoch 212, Loss: 0.725195199672613\n",
      "Epoch 213, Loss: 0.7249397366167222\n",
      "Epoch 214, Loss: 0.7246848108303051\n",
      "Epoch 215, Loss: 0.7244508001661418\n",
      "Epoch 216, Loss: 0.7242140098614814\n",
      "Epoch 217, Loss: 0.723983362035884\n",
      "Epoch 218, Loss: 0.7237679890059415\n",
      "Epoch 219, Loss: 0.7235461787283589\n",
      "Epoch 220, Loss: 0.7233334964583681\n",
      "Epoch 221, Loss: 0.7231248123527532\n",
      "Epoch 222, Loss: 0.7229249804183067\n",
      "Epoch 223, Loss: 0.7227206752291238\n",
      "Epoch 224, Loss: 0.7225032442609098\n",
      "Epoch 225, Loss: 0.7223222483952229\n",
      "Epoch 226, Loss: 0.7221255679153286\n",
      "Epoch 227, Loss: 0.7219154612485863\n",
      "Epoch 228, Loss: 0.7217377597091017\n",
      "Epoch 229, Loss: 0.7215414452656074\n",
      "Epoch 230, Loss: 0.7213513797614312\n",
      "Epoch 231, Loss: 0.7211650166604273\n",
      "Epoch 232, Loss: 0.7210071326512766\n",
      "Epoch 233, Loss: 0.7208108953741539\n",
      "Epoch 234, Loss: 0.7206491003421697\n",
      "Epoch 235, Loss: 0.7204653463663747\n",
      "Epoch 236, Loss: 0.7202920603534183\n",
      "Epoch 237, Loss: 0.7201332083817445\n",
      "Epoch 238, Loss: 0.719957171622035\n",
      "Epoch 239, Loss: 0.719779631962831\n",
      "Epoch 240, Loss: 0.7196235775741862\n",
      "Epoch 241, Loss: 0.7194667592329131\n",
      "Epoch 242, Loss: 0.7192999079466624\n",
      "Epoch 243, Loss: 0.7191487307098359\n",
      "Epoch 244, Loss: 0.7189955715816229\n",
      "Epoch 245, Loss: 0.7188315500242873\n",
      "Epoch 246, Loss: 0.7186720223851679\n",
      "Epoch 247, Loss: 0.718505217743948\n",
      "Epoch 248, Loss: 0.718357434815053\n",
      "Epoch 249, Loss: 0.7181993434268493\n",
      "Epoch 250, Loss: 0.718040474137981\n",
      "Epoch 251, Loss: 0.7179004877674096\n",
      "Epoch 252, Loss: 0.717773154732247\n",
      "Epoch 253, Loss: 0.7176048058157195\n",
      "Epoch 254, Loss: 0.7174726234833473\n",
      "Epoch 255, Loss: 0.7173034340142828\n",
      "Epoch 256, Loss: 0.7171598557788024\n",
      "Epoch 257, Loss: 0.7170205346970095\n",
      "Epoch 258, Loss: 0.7168791374429684\n",
      "Epoch 259, Loss: 0.7167856960246428\n",
      "Epoch 260, Loss: 0.7166364946940352\n",
      "Epoch 261, Loss: 0.7164839302370515\n",
      "Epoch 262, Loss: 0.7163229801905195\n",
      "Epoch 263, Loss: 0.7162134163455175\n",
      "Epoch 264, Loss: 0.7160602078258221\n",
      "Epoch 265, Loss: 0.7159208193582313\n",
      "Epoch 266, Loss: 0.7157813332473573\n",
      "Epoch 267, Loss: 0.715638579124384\n",
      "Epoch 268, Loss: 0.7155033611860778\n",
      "Epoch 269, Loss: 0.7153661088036026\n",
      "Epoch 270, Loss: 0.7152279285832189\n",
      "Epoch 271, Loss: 0.7150999047186374\n",
      "Epoch 272, Loss: 0.7149563224416362\n",
      "Epoch 273, Loss: 0.7148426953683383\n",
      "Epoch 274, Loss: 0.7146966505185632\n",
      "Epoch 275, Loss: 0.7145740913884413\n",
      "Epoch 276, Loss: 0.71442108491421\n",
      "Epoch 277, Loss: 0.714303247401873\n",
      "Epoch 278, Loss: 0.7141631728606026\n",
      "Epoch 279, Loss: 0.7140443417655283\n",
      "Epoch 280, Loss: 0.7139046649719432\n",
      "Epoch 281, Loss: 0.7137879267553512\n",
      "Epoch 282, Loss: 0.7136486686027916\n",
      "Epoch 283, Loss: 0.7135257608988371\n",
      "Epoch 284, Loss: 0.7133913013842347\n",
      "Epoch 285, Loss: 0.713269480812584\n",
      "Epoch 286, Loss: 0.7131356031298725\n",
      "Epoch 287, Loss: 0.7130138811564998\n",
      "Epoch 288, Loss: 0.7128820505738019\n",
      "Epoch 289, Loss: 0.7127506531085727\n",
      "Epoch 290, Loss: 0.7126352146408347\n",
      "Epoch 291, Loss: 0.7124963607681954\n",
      "Epoch 292, Loss: 0.7123727436180523\n",
      "Epoch 293, Loss: 0.7122573392879434\n",
      "Epoch 294, Loss: 0.7121244905595481\n",
      "Epoch 295, Loss: 0.7119936556034304\n",
      "Epoch 296, Loss: 0.7118798391890522\n",
      "Epoch 297, Loss: 0.7117457041979952\n",
      "Epoch 298, Loss: 0.7116279963008254\n",
      "Epoch 299, Loss: 0.7115026065145377\n",
      "Epoch 300, Loss: 0.7113735899599027\n",
      "Epoch 301, Loss: 0.7112422682140003\n",
      "Epoch 302, Loss: 0.7111100831689\n",
      "Epoch 303, Loss: 0.7109953216963228\n",
      "Epoch 304, Loss: 0.7108728664979466\n",
      "Epoch 305, Loss: 0.7107488249523739\n",
      "Epoch 306, Loss: 0.7106300111251244\n",
      "Epoch 307, Loss: 0.710509700555775\n",
      "Epoch 308, Loss: 0.7103826401172824\n",
      "Epoch 309, Loss: 0.7102630585475752\n",
      "Epoch 310, Loss: 0.7101426491214828\n",
      "Epoch 311, Loss: 0.7100240964712683\n",
      "Epoch 312, Loss: 0.7098930995973836\n",
      "Epoch 313, Loss: 0.709779268983352\n",
      "Epoch 314, Loss: 0.7096586762660507\n",
      "Epoch 315, Loss: 0.7095382231852931\n",
      "Epoch 316, Loss: 0.7094150904693023\n",
      "Epoch 317, Loss: 0.7092924513248845\n",
      "Epoch 318, Loss: 0.7091728046373462\n",
      "Epoch 319, Loss: 0.7090494375787851\n",
      "Epoch 320, Loss: 0.7089235881362375\n",
      "Epoch 321, Loss: 0.708821322579469\n",
      "Epoch 322, Loss: 0.7086955552728107\n",
      "Epoch 323, Loss: 0.7085761215264355\n",
      "Epoch 324, Loss: 0.7084529814880497\n",
      "Epoch 325, Loss: 0.7083320364613697\n",
      "Epoch 326, Loss: 0.7082148934178385\n",
      "Epoch 327, Loss: 0.7080942430757069\n",
      "Epoch 328, Loss: 0.7079814264455748\n",
      "Epoch 329, Loss: 0.7078560904254404\n",
      "Epoch 330, Loss: 0.7077361718477149\n",
      "Epoch 331, Loss: 0.7076212977935444\n",
      "Epoch 332, Loss: 0.707491927334514\n",
      "Epoch 333, Loss: 0.7073815125658737\n",
      "Epoch 334, Loss: 0.7072661209123846\n",
      "Epoch 335, Loss: 0.7071485238706906\n",
      "Epoch 336, Loss: 0.7070347519599048\n",
      "Epoch 337, Loss: 0.706914289961418\n",
      "Epoch 338, Loss: 0.7067945254905413\n",
      "Epoch 339, Loss: 0.7066811537002663\n",
      "Epoch 340, Loss: 0.7065527968378745\n",
      "Epoch 341, Loss: 0.7064392547532059\n",
      "Epoch 342, Loss: 0.7063255942153754\n",
      "Epoch 343, Loss: 0.7062015907337724\n",
      "Epoch 344, Loss: 0.7060888972483886\n",
      "Epoch 345, Loss: 0.7059682040403971\n",
      "Epoch 346, Loss: 0.7058569810722136\n",
      "Epoch 347, Loss: 0.7057422018058718\n",
      "Epoch 348, Loss: 0.7056238778015399\n",
      "Epoch 349, Loss: 0.7055121454771393\n",
      "Epoch 350, Loss: 0.7053983552112104\n",
      "Epoch 351, Loss: 0.7052827177107713\n",
      "Epoch 352, Loss: 0.7051626450391311\n",
      "Epoch 353, Loss: 0.7050450145287301\n",
      "Epoch 354, Loss: 0.7049135583103118\n",
      "Epoch 355, Loss: 0.7048155714768657\n",
      "Epoch 356, Loss: 0.704685029854355\n",
      "Epoch 357, Loss: 0.7045840691026811\n",
      "Epoch 358, Loss: 0.704457747033375\n",
      "Epoch 359, Loss: 0.704385882273217\n",
      "Epoch 360, Loss: 0.7042383716520183\n",
      "Epoch 361, Loss: 0.7041260912519103\n",
      "Epoch 362, Loss: 0.7040113098257343\n",
      "Epoch 363, Loss: 0.7038934601771196\n",
      "Epoch 364, Loss: 0.7037796756030926\n",
      "Epoch 365, Loss: 0.7036645710036994\n",
      "Epoch 366, Loss: 0.7035538815088286\n",
      "Epoch 367, Loss: 0.7034316813456682\n",
      "Epoch 368, Loss: 0.7033325736206599\n",
      "Epoch 369, Loss: 0.7032186531068554\n",
      "Epoch 370, Loss: 0.7031055897020249\n",
      "Epoch 371, Loss: 0.7029862271161942\n",
      "Epoch 372, Loss: 0.7028678717671666\n",
      "Epoch 373, Loss: 0.7027665849694604\n",
      "Epoch 374, Loss: 0.7026438204152756\n",
      "Epoch 375, Loss: 0.7025331452270434\n",
      "Epoch 376, Loss: 0.7024162748549206\n",
      "Epoch 377, Loss: 0.7023077972980208\n",
      "Epoch 378, Loss: 0.7021855223224159\n",
      "Epoch 379, Loss: 0.7020546201448586\n",
      "Epoch 380, Loss: 0.7019554315234667\n",
      "Epoch 381, Loss: 0.7018538298754569\n",
      "Epoch 382, Loss: 0.7017358904924753\n",
      "Epoch 383, Loss: 0.7016168252321314\n",
      "Epoch 384, Loss: 0.7015176881453017\n",
      "Epoch 385, Loss: 0.7013958025558449\n",
      "Epoch 386, Loss: 0.7012904512630916\n",
      "Epoch 387, Loss: 0.7011650577959525\n",
      "Epoch 388, Loss: 0.7010642065221647\n",
      "Epoch 389, Loss: 0.7009415473811363\n",
      "Epoch 390, Loss: 0.7008541066167894\n",
      "Epoch 391, Loss: 0.7007240143654265\n",
      "Epoch 392, Loss: 0.7006179563457146\n",
      "Epoch 393, Loss: 0.700476142452822\n",
      "Epoch 394, Loss: 0.7003813598315721\n",
      "Epoch 395, Loss: 0.7002640121235022\n",
      "Epoch 396, Loss: 0.7001611355320597\n",
      "Epoch 397, Loss: 0.7000388722293908\n",
      "Epoch 398, Loss: 0.6999193702769438\n",
      "Epoch 399, Loss: 0.6998257279301423\n",
      "Epoch 400, Loss: 0.699700685737508\n",
      "Epoch 401, Loss: 0.699594122620286\n",
      "Epoch 402, Loss: 0.6994782681413639\n",
      "Epoch 403, Loss: 0.6993718631197395\n",
      "Epoch 404, Loss: 0.6992253187674735\n",
      "Epoch 405, Loss: 0.6991543127325903\n",
      "Epoch 406, Loss: 0.6990400386478559\n",
      "Epoch 407, Loss: 0.6989222501386618\n",
      "Epoch 408, Loss: 0.6988084286769871\n",
      "Epoch 409, Loss: 0.69871112909464\n",
      "Epoch 410, Loss: 0.698598147325636\n",
      "Epoch 411, Loss: 0.6984680855148656\n",
      "Epoch 412, Loss: 0.6983822644538079\n",
      "Epoch 413, Loss: 0.6982369400243053\n",
      "Epoch 414, Loss: 0.6981786552637692\n",
      "Epoch 415, Loss: 0.6980067503126517\n",
      "Epoch 416, Loss: 0.6979105481486491\n",
      "Epoch 417, Loss: 0.6978110284299996\n",
      "Epoch 418, Loss: 0.6976964412082409\n",
      "Epoch 419, Loss: 0.6975728018234071\n",
      "Epoch 420, Loss: 0.6974600679934772\n",
      "Epoch 421, Loss: 0.6973654828858264\n",
      "Epoch 422, Loss: 0.6972371882638682\n",
      "Epoch 423, Loss: 0.697128255568647\n",
      "Epoch 424, Loss: 0.6970499387417827\n",
      "Epoch 425, Loss: 0.6968776496155467\n",
      "Epoch 426, Loss: 0.6967958610354554\n",
      "Epoch 427, Loss: 0.6966968108445664\n",
      "Epoch 428, Loss: 0.6965629252639723\n",
      "Epoch 429, Loss: 0.6964600978186132\n",
      "Epoch 430, Loss: 0.6963195907652213\n",
      "Epoch 431, Loss: 0.6962402789093656\n",
      "Epoch 432, Loss: 0.6960800450491199\n",
      "Epoch 433, Loss: 0.6960187109503638\n",
      "Epoch 434, Loss: 0.6958705281792688\n",
      "Epoch 435, Loss: 0.6957891026521165\n",
      "Epoch 436, Loss: 0.6956432041142846\n",
      "Epoch 437, Loss: 0.6955485918532764\n",
      "Epoch 438, Loss: 0.6953910769194338\n",
      "Epoch 439, Loss: 0.695329762632425\n",
      "Epoch 440, Loss: 0.6952301050451901\n",
      "Epoch 441, Loss: 0.6950368204787568\n",
      "Epoch 442, Loss: 0.6949785109917141\n",
      "Epoch 443, Loss: 0.6948188381641756\n",
      "Epoch 444, Loss: 0.694764917915684\n",
      "Epoch 445, Loss: 0.6946125190370598\n",
      "Epoch 446, Loss: 0.6944810161552789\n",
      "Epoch 447, Loss: 0.6944411751562825\n",
      "Epoch 448, Loss: 0.6942386004185982\n",
      "Epoch 449, Loss: 0.6941800699247757\n",
      "Epoch 450, Loss: 0.6940194634407025\n",
      "Epoch 451, Loss: 0.6939092429445842\n",
      "Epoch 452, Loss: 0.6938267076048329\n",
      "Epoch 453, Loss: 0.6936592370655007\n",
      "Epoch 454, Loss: 0.6936254393016325\n",
      "Epoch 455, Loss: 0.6934338044790973\n",
      "Epoch 456, Loss: 0.6933435360116011\n",
      "Epoch 457, Loss: 0.6932215750608052\n",
      "Epoch 458, Loss: 0.6931266401816484\n",
      "Epoch 459, Loss: 0.6929949158330525\n",
      "Epoch 460, Loss: 0.6928734182353178\n",
      "Epoch 461, Loss: 0.69276264064759\n",
      "Epoch 462, Loss: 0.6926146532426997\n",
      "Epoch 463, Loss: 0.692541048417778\n",
      "Epoch 464, Loss: 0.6923956119327764\n",
      "Epoch 465, Loss: 0.6923076724065055\n",
      "Epoch 466, Loss: 0.6921507158336305\n",
      "Epoch 467, Loss: 0.6920590231442135\n",
      "Epoch 468, Loss: 0.6919433082920119\n",
      "Epoch 469, Loss: 0.6917962689674901\n",
      "Epoch 470, Loss: 0.691700727954816\n",
      "Epoch 471, Loss: 0.6915684670687583\n",
      "Epoch 472, Loss: 0.6914198487538482\n",
      "Epoch 473, Loss: 0.6913567022910253\n",
      "Epoch 474, Loss: 0.6911929680875813\n",
      "Epoch 475, Loss: 0.6910185628149903\n",
      "Epoch 476, Loss: 0.6910174041680268\n",
      "Epoch 477, Loss: 0.6908352884866871\n",
      "Epoch 478, Loss: 0.6907106542669647\n",
      "Epoch 479, Loss: 0.6906198016938395\n",
      "Epoch 480, Loss: 0.6904563600041962\n",
      "Epoch 481, Loss: 0.6903695191422344\n",
      "Epoch 482, Loss: 0.6902366714972923\n",
      "Epoch 483, Loss: 0.6901015770865054\n",
      "Epoch 484, Loss: 0.689994437855365\n",
      "Epoch 485, Loss: 0.6898895632310221\n",
      "Epoch 486, Loss: 0.6897276316820278\n",
      "Epoch 487, Loss: 0.6896081942135003\n",
      "Epoch 488, Loss: 0.6894978519147437\n",
      "Epoch 489, Loss: 0.6893746753534095\n",
      "Epoch 490, Loss: 0.689232172566216\n",
      "Epoch 491, Loss: 0.6890869086886534\n",
      "Epoch 492, Loss: 0.6890105451127249\n",
      "Epoch 493, Loss: 0.6888542175124108\n",
      "Epoch 494, Loss: 0.688726353462952\n",
      "Epoch 495, Loss: 0.6886227798458298\n",
      "Epoch 496, Loss: 0.6884491669660766\n",
      "Epoch 497, Loss: 0.6883203096623259\n",
      "Epoch 498, Loss: 0.688202015464175\n",
      "Epoch 499, Loss: 0.6881190004760298\n",
      "Epoch 500, Loss: 0.6879543847298465\n",
      "Epoch 501, Loss: 0.6877977034629982\n",
      "Epoch 502, Loss: 0.687695214182105\n",
      "Epoch 503, Loss: 0.6875780979113716\n",
      "Epoch 504, Loss: 0.6874255205319879\n",
      "Epoch 505, Loss: 0.6873117206492517\n",
      "Epoch 506, Loss: 0.6871859808090987\n",
      "Epoch 507, Loss: 0.6870416399100813\n",
      "Epoch 508, Loss: 0.6869519353648138\n",
      "Epoch 509, Loss: 0.6867330228367893\n",
      "Epoch 510, Loss: 0.6866489319446559\n",
      "Epoch 511, Loss: 0.6865147604879863\n",
      "Epoch 512, Loss: 0.6863862730173484\n",
      "Epoch 513, Loss: 0.6862361148111344\n",
      "Epoch 514, Loss: 0.6861364734670308\n",
      "Epoch 515, Loss: 0.6859692932870645\n",
      "Epoch 516, Loss: 0.6858230737068483\n",
      "Epoch 517, Loss: 0.6857301277289386\n",
      "Epoch 518, Loss: 0.6855613143889521\n",
      "Epoch 519, Loss: 0.6854340487350475\n",
      "Epoch 520, Loss: 0.685323602323926\n",
      "Epoch 521, Loss: 0.6851317560819026\n",
      "Epoch 522, Loss: 0.6850214890043689\n",
      "Epoch 523, Loss: 0.6848744358864899\n",
      "Epoch 524, Loss: 0.6847245345545334\n",
      "Epoch 525, Loss: 0.6845737657485049\n",
      "Epoch 526, Loss: 0.684464213367092\n",
      "Epoch 527, Loss: 0.6843367252616913\n",
      "Epoch 528, Loss: 0.6841677878352768\n",
      "Epoch 529, Loss: 0.6840274301481085\n",
      "Epoch 530, Loss: 0.6838709200060061\n",
      "Epoch 531, Loss: 0.6837499164365362\n",
      "Epoch 532, Loss: 0.6835984981460201\n",
      "Epoch 533, Loss: 0.6834695033136461\n",
      "Epoch 534, Loss: 0.6833180354542286\n",
      "Epoch 535, Loss: 0.6831492831692836\n",
      "Epoch 536, Loss: 0.6830381971383711\n",
      "Epoch 537, Loss: 0.6828705315701168\n",
      "Epoch 538, Loss: 0.6827407543589316\n",
      "Epoch 539, Loss: 0.6825514716853663\n",
      "Epoch 540, Loss: 0.6824123544792473\n",
      "Epoch 541, Loss: 0.6822877950828612\n",
      "Epoch 542, Loss: 0.6821195516960847\n",
      "Epoch 543, Loss: 0.6819833817247589\n",
      "Epoch 544, Loss: 0.6817989238919556\n",
      "Epoch 545, Loss: 0.6816781592199952\n",
      "Epoch 546, Loss: 0.6815025913807077\n",
      "Epoch 547, Loss: 0.6813899738771465\n",
      "Epoch 548, Loss: 0.681174237268895\n",
      "Epoch 549, Loss: 0.6810778917120943\n",
      "Epoch 550, Loss: 0.6809007565375759\n",
      "Epoch 551, Loss: 0.6807308308420149\n",
      "Epoch 552, Loss: 0.6806002599583671\n",
      "Epoch 553, Loss: 0.6804184614357516\n",
      "Epoch 554, Loss: 0.6802662155928404\n",
      "Epoch 555, Loss: 0.680126555206596\n",
      "Epoch 556, Loss: 0.6799367596925422\n",
      "Epoch 557, Loss: 0.6797757867516839\n",
      "Epoch 558, Loss: 0.6796156573223641\n",
      "Epoch 559, Loss: 0.679421870918782\n",
      "Epoch 560, Loss: 0.6793576680554402\n",
      "Epoch 561, Loss: 0.679124535407373\n",
      "Epoch 562, Loss: 0.6789796161351052\n",
      "Epoch 563, Loss: 0.6788089302982688\n",
      "Epoch 564, Loss: 0.6786598536566993\n",
      "Epoch 565, Loss: 0.6784691669475218\n",
      "Epoch 566, Loss: 0.6783237411833173\n",
      "Epoch 567, Loss: 0.6781824334719174\n",
      "Epoch 568, Loss: 0.6779832083196473\n",
      "Epoch 569, Loss: 0.6778352957678229\n",
      "Epoch 570, Loss: 0.6776611348539292\n",
      "Epoch 571, Loss: 0.6774838263449318\n",
      "Epoch 572, Loss: 0.677323369490186\n",
      "Epoch 573, Loss: 0.67713671219275\n",
      "Epoch 574, Loss: 0.6769836939311132\n",
      "Epoch 575, Loss: 0.6767811626903953\n",
      "Epoch 576, Loss: 0.6766503258133292\n",
      "Epoch 577, Loss: 0.6764339904932314\n",
      "Epoch 578, Loss: 0.6763078092992252\n",
      "Epoch 579, Loss: 0.6760830721050136\n",
      "Epoch 580, Loss: 0.6759506881602267\n",
      "Epoch 581, Loss: 0.6757423307571193\n",
      "Epoch 582, Loss: 0.6755869141881791\n",
      "Epoch 583, Loss: 0.6753689648987853\n",
      "Epoch 584, Loss: 0.6752199645760002\n",
      "Epoch 585, Loss: 0.6750287194413391\n",
      "Epoch 586, Loss: 0.6748525605857746\n",
      "Epoch 587, Loss: 0.6746615834628424\n",
      "Epoch 588, Loss: 0.6744825595809986\n",
      "Epoch 589, Loss: 0.6743137991381076\n",
      "Epoch 590, Loss: 0.6741306055017611\n",
      "Epoch 591, Loss: 0.6738996130574108\n",
      "Epoch 592, Loss: 0.6737514565612407\n",
      "Epoch 593, Loss: 0.673548938539231\n",
      "Epoch 594, Loss: 0.6733765852721317\n",
      "Epoch 595, Loss: 0.6731669972978717\n",
      "Epoch 596, Loss: 0.6729732592432645\n",
      "Epoch 597, Loss: 0.6728274657414294\n",
      "Epoch 598, Loss: 0.6725931467098369\n",
      "Epoch 599, Loss: 0.6723897841177523\n",
      "Epoch 600, Loss: 0.6722597719580796\n",
      "Epoch 601, Loss: 0.6719914732160036\n",
      "Epoch 602, Loss: 0.6718046491283233\n",
      "Epoch 603, Loss: 0.6716535077319977\n",
      "Epoch 604, Loss: 0.6713919161021006\n",
      "Epoch 605, Loss: 0.671210699718961\n",
      "Epoch 606, Loss: 0.6710075987841494\n",
      "Epoch 607, Loss: 0.6708119286513964\n",
      "Epoch 608, Loss: 0.6705782304055107\n",
      "Epoch 609, Loss: 0.6704100054073003\n",
      "Epoch 610, Loss: 0.6702635669840082\n",
      "Epoch 611, Loss: 0.6699547831489799\n",
      "Epoch 612, Loss: 0.6697649800858542\n",
      "Epoch 613, Loss: 0.6695950072061186\n",
      "Epoch 614, Loss: 0.6693879634490035\n",
      "Epoch 615, Loss: 0.6691285137128437\n",
      "Epoch 616, Loss: 0.6689729401469521\n",
      "Epoch 617, Loss: 0.6687276732719312\n",
      "Epoch 618, Loss: 0.6685301149694374\n",
      "Epoch 619, Loss: 0.6682740294116767\n",
      "Epoch 620, Loss: 0.668116166773892\n",
      "Epoch 621, Loss: 0.6678519144867584\n",
      "Epoch 622, Loss: 0.6676443534315898\n",
      "Epoch 623, Loss: 0.6673884546994375\n",
      "Epoch 624, Loss: 0.667267433518225\n",
      "Epoch 625, Loss: 0.6669816440015055\n",
      "Epoch 626, Loss: 0.666749854931022\n",
      "Epoch 627, Loss: 0.6665339171606445\n",
      "Epoch 628, Loss: 0.66632900427934\n",
      "Epoch 629, Loss: 0.6660697215628889\n",
      "Epoch 630, Loss: 0.6658744726851398\n",
      "Epoch 631, Loss: 0.6656320460795542\n",
      "Epoch 632, Loss: 0.6654099690055161\n",
      "Epoch 633, Loss: 0.6651232757636386\n",
      "Epoch 634, Loss: 0.6649614642430951\n",
      "Epoch 635, Loss: 0.6647116853210303\n",
      "Epoch 636, Loss: 0.6644818581782779\n",
      "Epoch 637, Loss: 0.6642625999435107\n",
      "Epoch 638, Loss: 0.6639613706820098\n",
      "Epoch 639, Loss: 0.6637738248941613\n",
      "Epoch 640, Loss: 0.663508644324548\n",
      "Epoch 641, Loss: 0.6633121821932343\n",
      "Epoch 642, Loss: 0.6630135730399138\n",
      "Epoch 643, Loss: 0.6628083159101837\n",
      "Epoch 644, Loss: 0.6625197219141385\n",
      "Epoch 645, Loss: 0.6623520733005414\n",
      "Epoch 646, Loss: 0.6620299182560594\n",
      "Epoch 647, Loss: 0.6618099976231187\n",
      "Epoch 648, Loss: 0.6615010578959493\n",
      "Epoch 649, Loss: 0.6613306810814411\n",
      "Epoch 650, Loss: 0.6610662767983296\n",
      "Epoch 651, Loss: 0.6607666265040414\n",
      "Epoch 652, Loss: 0.6605249975883435\n",
      "Epoch 653, Loss: 0.6602911542687333\n",
      "Epoch 654, Loss: 0.6600322187466527\n",
      "Epoch 655, Loss: 0.6597812216666562\n",
      "Epoch 656, Loss: 0.6594362751626914\n",
      "Epoch 657, Loss: 0.6592870339611085\n",
      "Epoch 658, Loss: 0.6589321464048333\n",
      "Epoch 659, Loss: 0.6587422324958355\n",
      "Epoch 660, Loss: 0.6584045775392635\n",
      "Epoch 661, Loss: 0.6581930567322077\n",
      "Epoch 662, Loss: 0.6578650744126222\n",
      "Epoch 663, Loss: 0.6576289722086266\n",
      "Epoch 664, Loss: 0.6573723635595567\n",
      "Epoch 665, Loss: 0.6570716148544243\n",
      "Epoch 666, Loss: 0.6568175648215724\n",
      "Epoch 667, Loss: 0.6565295092838407\n",
      "Epoch 668, Loss: 0.6562366839462743\n",
      "Epoch 669, Loss: 0.6559725200110104\n",
      "Epoch 670, Loss: 0.6557149162760345\n",
      "Epoch 671, Loss: 0.6554000606130684\n",
      "Epoch 672, Loss: 0.655119953399209\n",
      "Epoch 673, Loss: 0.6548415250900246\n",
      "Epoch 674, Loss: 0.65458154881284\n",
      "Epoch 675, Loss: 0.6542409558690664\n",
      "Epoch 676, Loss: 0.6539942988690756\n",
      "Epoch 677, Loss: 0.6536990738198559\n",
      "Epoch 678, Loss: 0.6533955312792321\n",
      "Epoch 679, Loss: 0.6531055997294412\n",
      "Epoch 680, Loss: 0.652820907143518\n",
      "Epoch 681, Loss: 0.6525096154251511\n",
      "Epoch 682, Loss: 0.6522597589385413\n",
      "Epoch 683, Loss: 0.6519317894517858\n",
      "Epoch 684, Loss: 0.6515991438592942\n",
      "Epoch 685, Loss: 0.6513246689910592\n",
      "Epoch 686, Loss: 0.6510543411931928\n",
      "Epoch 687, Loss: 0.650669890037472\n",
      "Epoch 688, Loss: 0.650420218217699\n",
      "Epoch 689, Loss: 0.6500999724536322\n",
      "Epoch 690, Loss: 0.6497710239079272\n",
      "Epoch 691, Loss: 0.6494733214808949\n",
      "Epoch 692, Loss: 0.6491794798216178\n",
      "Epoch 693, Loss: 0.6488171658133548\n",
      "Epoch 694, Loss: 0.6485507914510105\n",
      "Epoch 695, Loss: 0.6482144929172556\n",
      "Epoch 696, Loss: 0.6479002177926804\n",
      "Epoch 697, Loss: 0.6475617325243861\n",
      "Epoch 698, Loss: 0.6472751479628356\n",
      "Epoch 699, Loss: 0.6469226248404865\n",
      "Epoch 700, Loss: 0.6465786558190479\n",
      "Epoch 701, Loss: 0.6462584378650328\n",
      "Epoch 702, Loss: 0.6459779149869644\n",
      "Epoch 703, Loss: 0.6456092476515972\n",
      "Epoch 704, Loss: 0.6452646118561165\n",
      "Epoch 705, Loss: 0.6449438403805341\n",
      "Epoch 706, Loss: 0.6446259708062312\n",
      "Epoch 707, Loss: 0.6442696774500228\n",
      "Epoch 708, Loss: 0.6439305174807989\n",
      "Epoch 709, Loss: 0.6435967921688127\n",
      "Epoch 710, Loss: 0.6432709023009243\n",
      "Epoch 711, Loss: 0.6429066214364609\n",
      "Epoch 712, Loss: 0.6425667540505624\n",
      "Epoch 713, Loss: 0.6422101150989045\n",
      "Epoch 714, Loss: 0.6419253294170765\n",
      "Epoch 715, Loss: 0.6415337877969683\n",
      "Epoch 716, Loss: 0.6411736297677193\n",
      "Epoch 717, Loss: 0.6408002491937798\n",
      "Epoch 718, Loss: 0.6405249005144533\n",
      "Epoch 719, Loss: 0.6400945888099882\n",
      "Epoch 720, Loss: 0.63979100789463\n",
      "Epoch 721, Loss: 0.6393892897059673\n",
      "Epoch 722, Loss: 0.6390483739200966\n",
      "Epoch 723, Loss: 0.638662056536552\n",
      "Epoch 724, Loss: 0.6383328283044729\n",
      "Epoch 725, Loss: 0.6379249662104324\n",
      "Epoch 726, Loss: 0.637622460237784\n",
      "Epoch 727, Loss: 0.6371978684936448\n",
      "Epoch 728, Loss: 0.6368366243458451\n",
      "Epoch 729, Loss: 0.6364515060847679\n",
      "Epoch 730, Loss: 0.6361033820483614\n",
      "Epoch 731, Loss: 0.635742824358813\n",
      "Epoch 732, Loss: 0.6353360786914625\n",
      "Epoch 733, Loss: 0.634995837112659\n",
      "Epoch 734, Loss: 0.634536869461673\n",
      "Epoch 735, Loss: 0.6342186141078722\n",
      "Epoch 736, Loss: 0.6337835127901396\n",
      "Epoch 737, Loss: 0.6334528878558819\n",
      "Epoch 738, Loss: 0.6330492542282475\n",
      "Epoch 739, Loss: 0.632618422009969\n",
      "Epoch 740, Loss: 0.6323088747858894\n",
      "Epoch 741, Loss: 0.6318904464511881\n",
      "Epoch 742, Loss: 0.631522606558865\n",
      "Epoch 743, Loss: 0.6310767446273954\n",
      "Epoch 744, Loss: 0.6307188863551584\n",
      "Epoch 745, Loss: 0.6303339611093418\n",
      "Epoch 746, Loss: 0.6299147710530238\n",
      "Epoch 747, Loss: 0.6294949989289641\n",
      "Epoch 748, Loss: 0.6291693097887914\n",
      "Epoch 749, Loss: 0.6286846459359293\n",
      "Epoch 750, Loss: 0.6283075363281863\n",
      "Epoch 751, Loss: 0.6279372737760652\n",
      "Epoch 752, Loss: 0.6275179178519716\n",
      "Epoch 753, Loss: 0.6270947930736905\n",
      "Epoch 754, Loss: 0.6266722454206637\n",
      "Epoch 755, Loss: 0.6263126796869078\n",
      "Epoch 756, Loss: 0.625828025509879\n",
      "Epoch 757, Loss: 0.6254742261835834\n",
      "Epoch 758, Loss: 0.6250143290233615\n",
      "Epoch 759, Loss: 0.62461299059526\n",
      "Epoch 760, Loss: 0.6241938525186878\n",
      "Epoch 761, Loss: 0.6238004339834676\n",
      "Epoch 762, Loss: 0.6233472994572131\n",
      "Epoch 763, Loss: 0.6229660858855894\n",
      "Epoch 764, Loss: 0.6224774633196712\n",
      "Epoch 765, Loss: 0.6220701169059536\n",
      "Epoch 766, Loss: 0.621686621272444\n",
      "Epoch 767, Loss: 0.6211939282210198\n",
      "Epoch 768, Loss: 0.6208539619304\n",
      "Epoch 769, Loss: 0.6202888878399135\n",
      "Epoch 770, Loss: 0.6199659441886108\n",
      "Epoch 771, Loss: 0.6195069848656661\n",
      "Epoch 772, Loss: 0.6190840026139138\n",
      "Epoch 773, Loss: 0.6185907139221049\n",
      "Epoch 774, Loss: 0.6181576504758276\n",
      "Epoch 775, Loss: 0.6177782249216875\n",
      "Epoch 776, Loss: 0.6172418703627444\n",
      "Epoch 777, Loss: 0.6168817076179571\n",
      "Epoch 778, Loss: 0.6164524343221436\n",
      "Epoch 779, Loss: 0.615906405615212\n",
      "Epoch 780, Loss: 0.6154956486384229\n",
      "Epoch 781, Loss: 0.6151048487562252\n",
      "Epoch 782, Loss: 0.6146370601992046\n",
      "Epoch 783, Loss: 0.6141583404175289\n",
      "Epoch 784, Loss: 0.6136835081656791\n",
      "Epoch 785, Loss: 0.6132566669588067\n",
      "Epoch 786, Loss: 0.6127816625728484\n",
      "Epoch 787, Loss: 0.6123635214678398\n",
      "Epoch 788, Loss: 0.6118677419505683\n",
      "Epoch 789, Loss: 0.6114051345618793\n",
      "Epoch 790, Loss: 0.6109354857145206\n",
      "Epoch 791, Loss: 0.6105120267852185\n",
      "Epoch 792, Loss: 0.6100788757690742\n",
      "Epoch 793, Loss: 0.6094997923331226\n",
      "Epoch 794, Loss: 0.6090869814213875\n",
      "Epoch 795, Loss: 0.6086364195353186\n",
      "Epoch 796, Loss: 0.6081273809944516\n",
      "Epoch 797, Loss: 0.6076768510371553\n",
      "Epoch 798, Loss: 0.6072144617608839\n",
      "Epoch 799, Loss: 0.6067276651494901\n",
      "Epoch 800, Loss: 0.6062416842557435\n",
      "Epoch 801, Loss: 0.6057420466631925\n",
      "Epoch 802, Loss: 0.6052993258923896\n",
      "Epoch 803, Loss: 0.6047648840503422\n",
      "Epoch 804, Loss: 0.6043644002260495\n",
      "Epoch 805, Loss: 0.6038524981342852\n",
      "Epoch 806, Loss: 0.6033050054600986\n",
      "Epoch 807, Loss: 0.6028805106671771\n",
      "Epoch 808, Loss: 0.6023573388895118\n",
      "Epoch 809, Loss: 0.6019178615704416\n",
      "Epoch 810, Loss: 0.6013833448601689\n",
      "Epoch 811, Loss: 0.6009256304695852\n",
      "Epoch 812, Loss: 0.6004190794120569\n",
      "Epoch 813, Loss: 0.5999009578658734\n",
      "Epoch 814, Loss: 0.599402687682631\n",
      "Epoch 815, Loss: 0.598945218918794\n",
      "Epoch 816, Loss: 0.5984076264274217\n",
      "Epoch 817, Loss: 0.5979703855616979\n",
      "Epoch 818, Loss: 0.5974080151547462\n",
      "Epoch 819, Loss: 0.5969095083353441\n",
      "Epoch 820, Loss: 0.5964383806810815\n",
      "Epoch 821, Loss: 0.5959224614897316\n",
      "Epoch 822, Loss: 0.5953909809171969\n",
      "Epoch 823, Loss: 0.5949217985083782\n",
      "Epoch 824, Loss: 0.5943989907622103\n",
      "Epoch 825, Loss: 0.5938880826303846\n",
      "Epoch 826, Loss: 0.5933678798957923\n",
      "Epoch 827, Loss: 0.5928440158695395\n",
      "Epoch 828, Loss: 0.5923669964749698\n",
      "Epoch 829, Loss: 0.5917969542680738\n",
      "Epoch 830, Loss: 0.5913369430167286\n",
      "Epoch 831, Loss: 0.590795797532359\n",
      "Epoch 832, Loss: 0.5902468132398675\n",
      "Epoch 833, Loss: 0.5897925153607239\n",
      "Epoch 834, Loss: 0.5892421464106704\n",
      "Epoch 835, Loss: 0.588679554234562\n",
      "Epoch 836, Loss: 0.5882262706683412\n",
      "Epoch 837, Loss: 0.5876770542288292\n",
      "Epoch 838, Loss: 0.5871200503429364\n",
      "Epoch 839, Loss: 0.5866031346576183\n",
      "Epoch 840, Loss: 0.5860912233131299\n",
      "Epoch 841, Loss: 0.5855748140509616\n",
      "Epoch 842, Loss: 0.5850356805389579\n",
      "Epoch 843, Loss: 0.5845235621159595\n",
      "Epoch 844, Loss: 0.5840045318659786\n",
      "Epoch 845, Loss: 0.5834757846717511\n",
      "Epoch 846, Loss: 0.5829036702615107\n",
      "Epoch 847, Loss: 0.5824299993915945\n",
      "Epoch 848, Loss: 0.5818977060575783\n",
      "Epoch 849, Loss: 0.5813065910438546\n",
      "Epoch 850, Loss: 0.5808283956317907\n",
      "Epoch 851, Loss: 0.5802862866378946\n",
      "Epoch 852, Loss: 0.5797087685418078\n",
      "Epoch 853, Loss: 0.5792211369499544\n",
      "Epoch 854, Loss: 0.5787071284698119\n",
      "Epoch 855, Loss: 0.5780891720271256\n",
      "Epoch 856, Loss: 0.5776193774377802\n",
      "Epoch 857, Loss: 0.5770801240015211\n",
      "Epoch 858, Loss: 0.5764544792666592\n",
      "Epoch 859, Loss: 0.5760071219936038\n",
      "Epoch 860, Loss: 0.5754512557916842\n",
      "Epoch 861, Loss: 0.5748669603777619\n",
      "Epoch 862, Loss: 0.5743902517148655\n",
      "Epoch 863, Loss: 0.5737778670276643\n",
      "Epoch 864, Loss: 0.5732987250317272\n",
      "Epoch 865, Loss: 0.5727104282815643\n",
      "Epoch 866, Loss: 0.5721515017914168\n",
      "Epoch 867, Loss: 0.5716424695302351\n",
      "Epoch 868, Loss: 0.5710966228523019\n",
      "Epoch 869, Loss: 0.570506650276518\n",
      "Epoch 870, Loss: 0.5699832999082536\n",
      "Epoch 871, Loss: 0.5694316952273236\n",
      "Epoch 872, Loss: 0.5688788944930241\n",
      "Epoch 873, Loss: 0.5683310506151219\n",
      "Epoch 874, Loss: 0.5677661611015392\n",
      "Epoch 875, Loss: 0.5672279370429417\n",
      "Epoch 876, Loss: 0.5667053705798203\n",
      "Epoch 877, Loss: 0.5661055306874364\n",
      "Epoch 878, Loss: 0.5655471004041702\n",
      "Epoch 879, Loss: 0.5649805285601769\n",
      "Epoch 880, Loss: 0.5644399199090775\n",
      "Epoch 881, Loss: 0.5638838734702912\n",
      "Epoch 882, Loss: 0.5633234604237403\n",
      "Epoch 883, Loss: 0.562775375841485\n",
      "Epoch 884, Loss: 0.5622147651209097\n",
      "Epoch 885, Loss: 0.5616230058740349\n",
      "Epoch 886, Loss: 0.5611380417336442\n",
      "Epoch 887, Loss: 0.5605332685443164\n",
      "Epoch 888, Loss: 0.5599495134706303\n",
      "Epoch 889, Loss: 0.5594436828732477\n",
      "Epoch 890, Loss: 0.5588837037085947\n",
      "Epoch 891, Loss: 0.5582868269160717\n",
      "Epoch 892, Loss: 0.5577584773241189\n",
      "Epoch 893, Loss: 0.5571934101337231\n",
      "Epoch 894, Loss: 0.5566786665133695\n",
      "Epoch 895, Loss: 0.5560811595857509\n",
      "Epoch 896, Loss: 0.5554977050433009\n",
      "Epoch 897, Loss: 0.5549546793699833\n",
      "Epoch 898, Loss: 0.5544146644674703\n",
      "Epoch 899, Loss: 0.553793232832514\n",
      "Epoch 900, Loss: 0.553256868129272\n",
      "Epoch 901, Loss: 0.5526874228356246\n",
      "Epoch 902, Loss: 0.5521535895303724\n",
      "Epoch 903, Loss: 0.5515759907038998\n",
      "Epoch 904, Loss: 0.5509833134176501\n",
      "Epoch 905, Loss: 0.5504735421256335\n",
      "Epoch 906, Loss: 0.5498966800397078\n",
      "Epoch 907, Loss: 0.5493442648071653\n",
      "Epoch 908, Loss: 0.5487819031007315\n",
      "Epoch 909, Loss: 0.5481762169008838\n",
      "Epoch 910, Loss: 0.5476458397296291\n",
      "Epoch 911, Loss: 0.547090343955161\n",
      "Epoch 912, Loss: 0.5464982893911967\n",
      "Epoch 913, Loss: 0.5459387385219182\n",
      "Epoch 914, Loss: 0.5453781470761739\n",
      "Epoch 915, Loss: 0.5448058706889003\n",
      "Epoch 916, Loss: 0.5442750273272091\n",
      "Epoch 917, Loss: 0.5437070357082011\n",
      "Epoch 918, Loss: 0.5431365925547215\n",
      "Epoch 919, Loss: 0.5425816022021593\n",
      "Epoch 920, Loss: 0.5419983992956473\n",
      "Epoch 921, Loss: 0.541441322976093\n",
      "Epoch 922, Loss: 0.540877786013428\n",
      "Epoch 923, Loss: 0.5402843121883184\n",
      "Epoch 924, Loss: 0.539742801531102\n",
      "Epoch 925, Loss: 0.5391944927826905\n",
      "Epoch 926, Loss: 0.5386309810745222\n",
      "Epoch 927, Loss: 0.5380436712524068\n",
      "Epoch 928, Loss: 0.5374905922583681\n",
      "Epoch 929, Loss: 0.5369288368028275\n",
      "Epoch 930, Loss: 0.5363843143979197\n",
      "Epoch 931, Loss: 0.5358134761049195\n",
      "Epoch 932, Loss: 0.5352430028878445\n",
      "Epoch 933, Loss: 0.5346807372507427\n",
      "Epoch 934, Loss: 0.534090231991965\n",
      "Epoch 935, Loss: 0.5335558709624593\n",
      "Epoch 936, Loss: 0.5330143607185334\n",
      "Epoch 937, Loss: 0.5324150426442726\n",
      "Epoch 938, Loss: 0.5318445203177508\n",
      "Epoch 939, Loss: 0.5312277811476445\n",
      "Epoch 940, Loss: 0.5307023495197487\n",
      "Epoch 941, Loss: 0.5301280962080802\n",
      "Epoch 942, Loss: 0.5295684943689521\n",
      "Epoch 943, Loss: 0.529021354509452\n",
      "Epoch 944, Loss: 0.528430363662037\n",
      "Epoch 945, Loss: 0.5278604272627156\n",
      "Epoch 946, Loss: 0.527317041604248\n",
      "Epoch 947, Loss: 0.5268401647921163\n",
      "Epoch 948, Loss: 0.526181190846264\n",
      "Epoch 949, Loss: 0.5256642261003829\n",
      "Epoch 950, Loss: 0.5250825700043543\n",
      "Epoch 951, Loss: 0.5245947880260456\n",
      "Epoch 952, Loss: 0.5240211286795028\n",
      "Epoch 953, Loss: 0.5234258428496836\n",
      "Epoch 954, Loss: 0.5228349042495425\n",
      "Epoch 955, Loss: 0.5223809439269299\n",
      "Epoch 956, Loss: 0.5217443369937018\n",
      "Epoch 957, Loss: 0.5211959778672308\n",
      "Epoch 958, Loss: 0.5206442639743679\n",
      "Epoch 959, Loss: 0.5200989870531898\n",
      "Epoch 960, Loss: 0.5195765379276449\n",
      "Epoch 961, Loss: 0.5190104195212162\n",
      "Epoch 962, Loss: 0.5184240755860565\n",
      "Epoch 963, Loss: 0.5178723194619687\n",
      "Epoch 964, Loss: 0.5173749379432989\n",
      "Epoch 965, Loss: 0.516778592711812\n",
      "Epoch 966, Loss: 0.5162020484308134\n",
      "Epoch 967, Loss: 0.5156930363156859\n",
      "Epoch 968, Loss: 0.5151458416388408\n",
      "Epoch 969, Loss: 0.5146172813357139\n",
      "Epoch 970, Loss: 0.5140165119672918\n",
      "Epoch 971, Loss: 0.5135188035691256\n",
      "Epoch 972, Loss: 0.5129037606575201\n",
      "Epoch 973, Loss: 0.5124212935133171\n",
      "Epoch 974, Loss: 0.5118241348387103\n",
      "Epoch 975, Loss: 0.51129186639707\n",
      "Epoch 976, Loss: 0.5107184806201307\n",
      "Epoch 977, Loss: 0.5101862417052857\n",
      "Epoch 978, Loss: 0.5096919308074834\n",
      "Epoch 979, Loss: 0.509083602876482\n",
      "Epoch 980, Loss: 0.5085552053202153\n",
      "Epoch 981, Loss: 0.5079833260606729\n",
      "Epoch 982, Loss: 0.5075278268328506\n",
      "Epoch 983, Loss: 0.506934532543513\n",
      "Epoch 984, Loss: 0.5063903156399097\n",
      "Epoch 985, Loss: 0.5058046418349532\n",
      "Epoch 986, Loss: 0.5053819120408377\n",
      "Epoch 987, Loss: 0.5047497455938879\n",
      "Epoch 988, Loss: 0.504249487380697\n",
      "Epoch 989, Loss: 0.5036447415447242\n",
      "Epoch 990, Loss: 0.5031484858048362\n",
      "Epoch 991, Loss: 0.5026696310763851\n",
      "Epoch 992, Loss: 0.5020560949621005\n",
      "Epoch 993, Loss: 0.5015313175719915\n",
      "Epoch 994, Loss: 0.500983004628172\n",
      "Epoch 995, Loss: 0.5004888745591319\n",
      "Epoch 996, Loss: 0.4999293754095915\n",
      "Epoch 997, Loss: 0.4993716595261121\n",
      "Epoch 998, Loss: 0.4988281553076031\n",
      "Epoch 999, Loss: 0.4982517045131544\n",
      "Test Accuracy: 81.67%\n"
     ]
    }
   ],
   "source": [
    "#Define a simple model and apply it to spiral dataset\n",
    "def generate_spiral_data(n_points_per_class: int, n_classes: int):\n",
    "    x = []\n",
    "    y = []\n",
    "    for j in range(n_classes):\n",
    "        ix = range(n_points_per_class * j, n_points_per_class * (j + 1))\n",
    "        r = np.linspace(0.0, 1, n_points_per_class)\n",
    "        t = np.linspace(j * 4, (j + 1) * 4, n_points_per_class) + np.random.randn(n_points_per_class) * 0.2\n",
    "        x1 = r * np.sin(t)\n",
    "        x2 = r * np.cos(t)\n",
    "        x.append(np.c_[x1, x2])\n",
    "        y.append(np.full(n_points_per_class, j))\n",
    "    x = np.vstack(x)\n",
    "    y = np.hstack(y)\n",
    "    y_one_hot = np.eye(n_classes)[y]\n",
    "    return x, y_one_hot\n",
    "\n",
    "\n",
    "x_train, y_train = generate_spiral_data(100, 3)\n",
    "x_test, y_test = generate_spiral_data(20, 3)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(2, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 3),\n",
    "])\n",
    "loss_fn = CrossEntropySoftMax()\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "#batch_size = 32\n",
    "# Train the model\n",
    "train(model, loss_fn, optimizer, x_train, y_train, epochs=1000, batch_size=10)\n",
    "# Test the model\n",
    "y_pred = model.forward(x_test)\n",
    "predicted_labels = np.argmax(y_pred, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9758906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader classes to wrap the data and operate on batches\n",
    "class Dataset:\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert isinstance(index, (int, np.ndarray)), \"Index must be an integer or a numpy array.\"\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "class DataLoader:\n",
    "    def __init__(self, dataset: Dataset, indices = None, batch_size: int = 32, shuffle: bool = False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(dataset))\n",
    "        self.current_index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= len(self.dataset):\n",
    "            raise StopIteration\n",
    "        start_index = self.current_index\n",
    "        end_index = min(start_index + self.batch_size, len(self.dataset))\n",
    "        batch_indices = self.indices[start_index:end_index]\n",
    "        x_batch, y_batch = self.dataset[batch_indices]\n",
    "        self.current_index += self.batch_size\n",
    "        return x_batch, y_batch\n",
    "    \n",
    "    @staticmethod\n",
    "    def holdout_split(dataset: Dataset, test_size: float = 0.2):\n",
    "        \"\"\"\n",
    "        Splits the dataset into training and testing sets.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to split.\n",
    "            test_size (float): The proportion of the dataset to include in the test split.\n",
    "        Returns:\n",
    "            DataLoader: Loader for the training portion of the dataset.\n",
    "            DataLoader: Loader for the testing portion of the dataset.\n",
    "        \"\"\"\n",
    "        assert 0 < test_size < 1, \"test_size must be between 0 and 1.\"\n",
    "        indices = np.arange(len(dataset))\n",
    "        np.random.shuffle(indices)\n",
    "        split_index = int(len(dataset) * (1 - test_size))\n",
    "        train_indices = indices[:split_index]\n",
    "        test_indices = indices[split_index:]\n",
    "        return DataLoader(dataset, train_indices), DataLoader(dataset, test_indices)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "804e0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer that governs the training process, using the DataLoader, ValidationStrategy, and Optimizer\n",
    "class Trainer:\n",
    "    def __init__(self, model: Sequential, loss_fn: Loss, optimizer: Optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, dataset: Dataset, epochs: int = 1000):\n",
    "        # Split the dataset into training and validation sets\n",
    "        train_loader, val_loader = DataLoader.holdout_split(dataset,test_size=0.2)\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                # Forward pass\n",
    "                y_pred = self.model.forward(x_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.loss_fn.forward(y_batch, y_pred)\n",
    "                epoch_loss += loss*x_batch.shape[0]\n",
    "                # Backward pass\n",
    "                grad = self.loss_fn.backward(y_batch, y_pred)\n",
    "                self.model.backward(grad)\n",
    "                \n",
    "                # Update parameters\n",
    "                for layer in self.model.layers:\n",
    "                    if isinstance(layer, Linear):\n",
    "                        self.optimizer.step(layer.weights, layer.grad_weights)\n",
    "                        self.optimizer.step(layer.bias, layer.grad_bias)\n",
    "\n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            # Validate the model\n",
    "            val_loss = self.validate(val_loader)\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "            \n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        val_loss = 0\n",
    "        for x_val, y_val in val_loader:\n",
    "            y_val_pred = self.model.forward(x_val)\n",
    "            val_loss += self.loss_fn.forward(y_val, y_val_pred)\n",
    "        val_loss /= (len(val_loader.dataset) // val_loader.batch_size)\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "44698a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossValidator class to perform k-fold cross-validation and other validation strategies\n",
    "\n",
    "class CrossValidator:\n",
    "    def __init__(self, model: Sequential, loss_fn: Loss, optimizer: Optimizer, validation_strategy : str = \"holdout\"):\n",
    "        assert validation_strategy in [\"holdout\", \"k-fold\"], \"Validation strategy must be either 'holdout' or 'k-fold'.\"\n",
    "        self.validation_strategy = validation_strategy\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.validation_strategy = validation_strategy\n",
    "\n",
    "    def cross_validate(self, dataset: Dataset, epochs: int = 1000):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "50c2fc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.0966, Validation Loss: 1.1106\n",
      "Epoch 1, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 2, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 3, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 4, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 5, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 6, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 7, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 8, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 9, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 10, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 11, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 12, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 13, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 14, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 15, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 16, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 17, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 18, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 19, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 20, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 21, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 22, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 23, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 24, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 25, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 26, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 27, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 28, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 29, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 30, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 31, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 32, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 33, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 34, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 35, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 36, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 37, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 38, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 39, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 40, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 41, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 42, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 43, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 44, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 45, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 46, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 47, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 48, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 49, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 50, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 51, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 52, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 53, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 54, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 55, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 56, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 57, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 58, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 59, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 60, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 61, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 62, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 63, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 64, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 65, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 66, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 67, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 68, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 69, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 70, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 71, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 72, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 73, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 74, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 75, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 76, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 77, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 78, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 79, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 80, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 81, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 82, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 83, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 84, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 85, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 86, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 87, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 88, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 89, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 90, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 91, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 92, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 93, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 94, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 95, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 96, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 97, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 98, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 99, Loss: 0.0000, Validation Loss: 0.0000\n",
      "Test Accuracy: 33.33%\n"
     ]
    }
   ],
   "source": [
    "#putting everything together\n",
    "x_train, y_train = generate_spiral_data(1000, 3)\n",
    "x_test, y_test = generate_spiral_data(200, 3)\n",
    "dataset = Dataset(x_train, y_train)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(2, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 3),\n",
    "])\n",
    "loss_function = CrossEntropySoftMax()\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "trainer = Trainer(model, loss_function, optimizer)\n",
    "trainer.train(dataset, epochs=100)\n",
    "# Test the model\n",
    "y_pred = model.forward(x_test)\n",
    "predicted_labels = np.argmax(y_pred, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
