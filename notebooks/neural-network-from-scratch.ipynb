{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0738778f",
   "metadata": {},
   "source": [
    "# From Scratch: Building a Neural Network with NumPy\n",
    "\n",
    "This notebook contains an end-to-end implementation of a simple feedforward neural network using **only NumPy** â€” no deep learning frameworks involved. It aims to demystify the inner workings of neural networks by walking through each component step-by-step, with a strong focus on **clarity, interactivity, and visualization**.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Manual forward and backward passes** (no autograd)\n",
    "\n",
    "> *This project is inspired by university coursework, but developed independently from scratch to reinforce my understanding and extend the ideas further.*\n",
    "\n",
    "In case of any questions or comments, please contact me at ea.arseneva@gmail.com\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1afb4b",
   "metadata": {},
   "source": [
    "## Plans:\n",
    "### Data and Training\n",
    "- Early stopping mechanism to prevent overfitting    (4)\n",
    "- Implement k-fold cross-validation                  (6)\n",
    "- Learning rate scheduling                           (5)\n",
    "- Training on other toy datasets (e.g., digit recognition or synthetic classification)                                      (11)\n",
    "\n",
    "### Visualization and Interactivity\n",
    "- (DONE) ~~Decision boundary visualization during training~~    (2)\n",
    "- (DONE) ~~Loss and accuracy plots after training~~  (1)\n",
    "- (DONE) ~~Visual explanation of gradients and weight updates~~ (3)\n",
    "- Interactive sliders for hyperparameters (learning rate, network architecture)                                        (12)\n",
    "\n",
    "### Model Improvements\n",
    "- Batch normalization implementation                 (9)\n",
    "- Dropout layers for regularization                  (10)\n",
    "- Different weight initialization strategies         (8) \n",
    "- Additional optimizers (Adam, RMSprop)              (7)\n",
    "\n",
    "### Future Improvements (OPTIONAL)\n",
    "- Training history tracking and logging\n",
    "- Model checkpointing and saving\n",
    "- Comprehensive testing suite\n",
    "- Performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    '''\n",
    "    Compute softmax values for each sets of scores in x.\n",
    "    x: 2D array of shape (n_samples, n_classes)\n",
    "    return: 2D array of shape (n_samples, n_classes) with softmax probabilities\n",
    "    \n",
    "    The softmax function is defined as:\n",
    "    softmax(x_i) = exp(x_i) / sum(exp(x_j))\n",
    "    where x_i is the i-th element of the input vector x and the sum is over all elements in x.\n",
    "    '''\n",
    "    # Subtract max for numerical stability\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def generate_spiral_data(n_points_per_class: int, n_classes: int):\n",
    "    '''\n",
    "    Generate spiral data for classification.\n",
    "    n_points_per_class: number of points per class\n",
    "    n_classes: number of classes\n",
    "    return: tuple (X, y_one_hot)\n",
    "    X: 2D array of shape (n_samples, 2) with the data points\n",
    "    y_one_hot: 2D array of shape (n_samples, n_classes) with one-hot encoded labels\n",
    "    '''\n",
    "    x = []\n",
    "    y = []\n",
    "    for j in range(n_classes):\n",
    "        ix = range(n_points_per_class * j, n_points_per_class * (j + 1))\n",
    "        r = np.linspace(0.0, 1, n_points_per_class)\n",
    "        t = np.linspace(j * 4, (j + 1) * 4, n_points_per_class) + np.random.randn(n_points_per_class) * 0.2\n",
    "        x1 = r * np.sin(t)\n",
    "        x2 = r * np.cos(t)\n",
    "        x.append(np.c_[x1, x2])\n",
    "        y.append(np.full(n_points_per_class, j))\n",
    "    x = np.vstack(x)\n",
    "    y = np.hstack(y)\n",
    "    y_one_hot = np.eye(n_classes)[y]\n",
    "    return x, y_one_hot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define abstract classes for Layer, Loss, Optimizer\n",
    "\n",
    "class Layer(ABC):\n",
    "    '''\n",
    "    Abstract base class for all layers in the neural network.\n",
    "    Each layer should implement the forward and backward methods, \n",
    "    the instances store their input and output dimensions.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_dim=None, output_dim=None):\n",
    "        self._input_dim = input_dim\n",
    "        self._output_dim = output_dim\n",
    "        self.input = None\n",
    "\n",
    "    @property\n",
    "    def input_dim(self):\n",
    "        return self._input_dim\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self._output_dim\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass through the layer.\n",
    "        Args:\n",
    "            x (np.ndarray): Input data. The shape should be (batch_size, input_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Output data. The shape should be (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the layer.\n",
    "        Args:\n",
    "            grad (np.ndarray): Gradient of the loss with respect to the output. \n",
    "            The shape should be (batch_size, output_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the input.\n",
    "            The shape should be (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class Loss(ABC):\n",
    "    '''\n",
    "    Abstract base class for all loss functions.\n",
    "    Each loss function should implement the forward and backward methods.\n",
    "    '''\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "        \"\"\"\n",
    "        Forward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            tuple: A tuple containing the loss value and the predicted probabilities.\n",
    "            The first element is a scalar (loss value), and the second element is an array of shape (batch_size,).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray, probs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "            probs (np.ndarray): Predicted probabilities. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the predictions. The shape should be (batch_size,).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class Optimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Update the parameters based on the gradients.\n",
    "        Args:\n",
    "            params (np.ndarray): Parameters to be updated. The shape should be (num_params,).\n",
    "            grads (np.ndarray): Gradients of the loss with respect to the parameters. The shape should be (num_params,).\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Layer: Linear, ReLU, and Sequential\n",
    "\n",
    "class ReLU(Layer):\n",
    "    '''\n",
    "    ReLU layer.\n",
    "    Applies the ReLU activation function element-wise to the input.\n",
    "    The ReLU function is defined as f(x) = max(0, x).\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape == self.input.shape, f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of ReLU is 1 for positive inputs, 0 for negative inputs\n",
    "        return grad * (self.input > 0)\n",
    "    \n",
    "    \n",
    "class Linear(Layer):\n",
    "    '''\n",
    "    Linear layer.\n",
    "    Applies a linear transformation to the input data.\n",
    "    The transformation is defined as y = xW + b, where W is the weight matrix and b is the bias vector.\n",
    "    '''\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        assert input_dim > 0 and output_dim > 0, \"Input and output dimensions of a Linear layer must be positive integers.\"\n",
    "        super().__init__(input_dim, output_dim)\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None   \n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.weights.shape[0], f\"Input shape {x.shape} does not match expected shape (batch_size, {self.weights.shape[0]})\"\n",
    "        assert self.weights.shape[1] == self.bias.shape[1], f\"Weights shape {self.weights.shape} does not match bias shape {self.bias.shape}\"\n",
    "        self.input = x\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape[1] == self.bias.shape[1], f\"Gradient shape {grad.shape} does not match bias shape {self.bias.shape}\"\n",
    "        assert grad.shape[0] == self.input.shape[0], f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of the loss with respect to the input\n",
    "        grad_input = grad @ self.weights.T\n",
    "        # Gradient of the loss with respect to the weights and bias\n",
    "        self.grad_weights = self.input.T @ grad\n",
    "        self.grad_bias = np.sum(grad, axis=0, keepdims=True)\n",
    "        return grad_input \n",
    "    \n",
    "class Sequential(Layer):\n",
    "    '''\n",
    "    Sequential model.\n",
    "    A container for stacking layers in a linear fashion.\n",
    "    The input to the first layer is the input to the model, and the output of the last layer is the output of the model.\n",
    "    '''\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        super().__init__(layers[0].input_dim, layers[-1].output_dim)\n",
    "        self.__check_consistency__()\n",
    "\n",
    "    def __check_consistency__(self):\n",
    "        assert len(self.layers) > 1, \"Sequential model must have at least one layer.\"\n",
    "        assert self.layers[0].input_dim is not None, \"First layer input dimension must be specified.\"\n",
    "        assert self.layers[-1].output_dim is not None, \"Last layer output dimension must be specified.\"\n",
    "        assert self.layers[0].input_dim == self.input_dim, f\"First layer input dimension {self.layers[0].input_dim} does not match expected input dimension {self.input_dim}\"\n",
    "        assert self.layers[-1].output_dim == self.output_dim, f\"Last layer output dimension {self.layers[-1].output_dim} does not match expected output dimension {self.output_dim}\"\n",
    "        current_dim = self.input_dim\n",
    "        mismatch_list = []\n",
    "        for layer in self.layers:\n",
    "            if layer.input_dim != None:\n",
    "                if layer.input_dim != current_dim: \n",
    "                    mismatch_list.append(f\"Layer {layer.__class__.__name__} input dimension {layer.input_dim} does not match expected input dimension {current_dim}\")\n",
    "                current_dim = layer.output_dim\n",
    "        assert len(mismatch_list) == 0, f\"Layer dimension mismatch: {'\\n'.join(mismatch_list)}\"\n",
    "                        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.layers[0].input_dim, f\"Input shape {x.shape} does not match expected shape (batch_size, {self.layers[0].input_dim})\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "    \n",
    "    def summary(self) -> None:\n",
    "        \"\"\"Print a summary of the model architecture.\"\"\"\n",
    "        print(\"Model Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        total_params = 0\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Linear):\n",
    "                params = np.prod(layer.weights.shape) + np.prod(layer.bias.shape)\n",
    "                total_params += params\n",
    "                print(f\"Layer {i}: {layer.__class__.__name__}, \"\n",
    "                      f\"Input: {layer.input_dim}, Output: {layer.output_dim}, \"\n",
    "                      f\"Parameters: {params}\")\n",
    "            else:\n",
    "                print(f\"Layer {i}: {layer.__class__.__name__}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total parameters: {total_params}\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Loss: MeanSquaredError and CrossEntropy\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "    '''\n",
    "    Mean Squared Error (MSE) loss function.\n",
    "    It measures the average squared difference between the predicted and true values.\n",
    "    The MSE is defined as:\n",
    "    MSE = (1/n) * sum((y_true - y_pred)^2)\n",
    "    where n is the number of samples, y_true is the true labels, and y_pred is the predicted labels.\n",
    "    '''\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return np.mean(np.square(y_true - y_pred)), y_pred\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "    \n",
    "    \n",
    "class CrossEntropySoftMax(Loss):\n",
    "    '''\n",
    "    Cross-entropy loss function with softmax activation.\n",
    "    It is used for multi-class classification problems.\n",
    "    The cross-entropy loss is defined as:\n",
    "    CE = -sum(y_true * log(probs))\n",
    "    where y_true is the true labels (one-hot encoded) and probs is the predicted probabilities.\n",
    "    '''\n",
    "    \n",
    "    def forward(self, y_true: np.ndarray, y_pred_logits: np.ndarray) -> tuple:\n",
    "        '''\n",
    "        Forward pass through the cross-entropy loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels (one-hot encoded). The shape should be (batch_size, n_classes).\n",
    "            y_pred_logits (np.ndarray): Predicted logits. The shape should be (batch_size, n_classes).\n",
    "        Returns:\n",
    "            tuple: A tuple containing the loss value and the predicted probabilities.\n",
    "            The first element is a scalar (loss value), and the second element is an array of shape (batch_size, n_classes).\n",
    "        '''\n",
    "        assert y_true.shape == y_pred_logits.shape, f\"True labels shape {y_true.shape} does not match predicted logits shape {y_pred_logits.shape}\"\n",
    "        #apply softmax to the predictions\n",
    "        probs = softmax(y_pred_logits)\n",
    "        loss = -np.sum(y_true * np.log(probs + 1e-15)) / y_true.shape[0]\n",
    "        return loss, probs\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, probs: np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Backward pass through the cross-entropy loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels (one-hot encoded). The shape should be (batch_size, n_classes).\n",
    "            probs (np.ndarray): Predicted probabilities. The shape should be (batch_size, n_classes).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the predictions. The shape should be (batch_size, n_classes).\n",
    "        '''\n",
    "        \n",
    "        assert y_true.shape == probs.shape, f\"True labels shape {y_true.shape} does not match prediction shape {probs.shape}\"\n",
    "        return (probs - y_true)/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Optimizer: SGD\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    '''\n",
    "    Stochastic Gradient Descent (SGD) optimizer.\n",
    "    It updates the parameters using the gradients and a learning rate.\n",
    "    The update rule is defined as:\n",
    "    params = params - learning_rate * grads\n",
    "    where params are the parameters to be updated, learning_rate is the learning rate, and grads are the gradients.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 0.01):\n",
    "        assert learning_rate > 0, \"Learning rate must be a positive number.\"\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        assert params.shape == grads.shape, f\"Parameters shape {params.shape} does not match gradients shape {grads.shape}\"\n",
    "        params -= self.learning_rate * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything together in a training loop\n",
    "def train(model: Sequential, loss_fn: Loss, optimizer: Optimizer, x_train: np.ndarray, y_train: np.ndarray, batch_size: int = 32, epochs: int = 1000):\n",
    "    '''\n",
    "    Train the model using the specified loss function and optimizer. No cross-validation is performed.\n",
    "    The training loop consists of the following steps:\n",
    "    1. Forward pass: Compute the predicted labels using the model.\n",
    "    2. Compute the loss using the loss function.\n",
    "    3. Backward pass: Compute the gradients of the loss with respect to the model parameters.\n",
    "    4. Update the model parameters using the optimizer.\n",
    "    5. Repeat steps 1-4 for the specified number of epochs.\n",
    "    6. Print the loss every 100 epochs.\n",
    "    7. Return the trained model.\n",
    "    Args:\n",
    "        model (Sequential): The model to be trained.\n",
    "        loss_fn (Loss): The loss function to be used.\n",
    "        optimizer (Optimizer): The optimizer to be used.\n",
    "        x_train (np.ndarray): Training data. The shape should be (n_samples, n_features).\n",
    "        y_train (np.ndarray): Training labels. The shape should be (n_samples, n_classes).\n",
    "        batch_size (int): Batch size for training. Default is 32.\n",
    "        epochs (int): Number of epochs for training. Default is 1000.\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        # go in batches\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model.forward(x_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            batch_loss, probs = loss_fn.forward(y_batch, y_pred)\n",
    "            epoch_loss += batch_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            grad = loss_fn.backward(y_batch, probs)\n",
    "            model.backward(grad)\n",
    "            \n",
    "            # Update parameters\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, Linear):\n",
    "                    optimizer.step(layer.weights, layer.grad_weights)\n",
    "                    optimizer.step(layer.bias, layer.grad_bias)\n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= (x_train.shape[0] // batch_size)\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a simple model and apply it to spiral dataset\n",
    "model = Sequential([\n",
    "    Linear(2, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 3),\n",
    "])\n",
    "loss_fn = CrossEntropySoftMax()\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "\n",
    "# Train the model\n",
    "x_train, y_train = generate_spiral_data(100, 3)\n",
    "x_test, y_test = generate_spiral_data(20, 3)\n",
    "train(model, loss_fn, optimizer, x_train, y_train, epochs=1000, batch_size=10)\n",
    "# Test the model\n",
    "y_pred = model.forward(x_test)\n",
    "predicted_labels = np.argmax(y_pred, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader classes to wrap the data and operate on batches\n",
    "class Dataset:\n",
    "    '''\n",
    "    Dataset class to hold the data and labels.\n",
    "    It provides methods to access the data and labels by index.\n",
    "    '''\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert isinstance(index, (int, np.ndarray)), \"Index must be an integer or a numpy array.\"\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "class DataLoader:\n",
    "    '''\n",
    "    DataLoader class to load the data in batches.\n",
    "    It provides methods to iterate over the data in batches.\n",
    "    '''\n",
    "    def __init__(self, dataset: Dataset, indices = None, batch_size: int = 32, shuffle: bool = False) -> None:\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = indices if indices is not None else np.arange(len(dataset))\n",
    "        self.current_index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= len(self.indices):\n",
    "            raise StopIteration\n",
    "        start_index = self.current_index\n",
    "        end_index = min(start_index + self.batch_size, len(self.indices))\n",
    "        batch_indices = self.indices[start_index:end_index]\n",
    "        x_batch, y_batch = self.dataset[batch_indices]\n",
    "        self.current_index += self.batch_size\n",
    "        return x_batch, y_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "    \n",
    "    @staticmethod\n",
    "    def holdout_split(dataset: Dataset, test_size: float = 0.2):\n",
    "        \"\"\"\n",
    "        Splits the dataset into training and testing sets.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to split.\n",
    "            test_size (float): The proportion of the dataset to include in the test split.\n",
    "        Returns:\n",
    "            DataLoader: Loader for the training portion of the dataset.\n",
    "            DataLoader: Loader for the testing portion of the dataset.\n",
    "        \"\"\"\n",
    "        assert 0 < test_size < 1, \"test_size must be between 0 and 1.\"\n",
    "        indices = np.arange(len(dataset))\n",
    "        np.random.shuffle(indices)\n",
    "        split_index = int(len(dataset) * (1 - test_size))\n",
    "        train_indices = indices[:split_index]\n",
    "        test_indices = indices[split_index:]\n",
    "        return DataLoader(dataset, train_indices), DataLoader(dataset, test_indices)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca70c15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class TrainingVisualizer that stores the training history and plots the loss and accuracy\n",
    "\n",
    "class TrainingVisualizer:\n",
    "    '''\n",
    "    TrainingVisualizer class to store the training history and plot the loss and accuracy.\n",
    "    It provides methods to update the training history and plot the loss and accuracy.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.history = {\n",
    "            'loss': [],\n",
    "            'val_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_acc': []\n",
    "        }\n",
    "        self.grid = None\n",
    "        self.grid_coords = None\n",
    "        \n",
    "    def update(self, loss: float, val_loss: float, train_acc: float, val_acc: float):\n",
    "        '''\n",
    "        Update the training history with the current loss and accuracy.\n",
    "        Args:   \n",
    "            loss (float): Current loss.\n",
    "            val_loss (float): Current validation loss.\n",
    "            train_acc (float): Current training accuracy.\n",
    "            val_acc (float): Current validation accuracy.\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        self.history['loss'].append(loss)\n",
    "        self.history['val_loss'].append(val_loss)\n",
    "        self.history['train_acc'].append(train_acc)\n",
    "        self.history['val_acc'].append(val_acc)\n",
    "\n",
    "    def plot_metrics_history(self):\n",
    "        '''\n",
    "        Plot the training history.\n",
    "        It plots the loss and accuracy for both training and validation sets.\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        assert len(self.history['loss']) > 0, \"No training history to plot.\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        epochs = range(len(self.history['loss']))\n",
    "        # Plot loss\n",
    "        ax1.plot(epochs, self.history['loss'], 'b-', label='Train Loss')\n",
    "        ax1.plot(epochs, self.history['val_loss'], 'r-', label='Val Loss')\n",
    "        ax1.set_title('Loss')\n",
    "        ax1.set_xlabel('Epochs')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot accuracy\n",
    "        ax2.plot(epochs, self.history['train_acc'], 'b-', label='Train Accuracy')\n",
    "        ax2.plot(epochs, self.history['val_acc'], 'r-', label='Val Accuracy')\n",
    "        ax2.set_title('Accuracy')\n",
    "        ax2.set_xlabel('Epochs')\n",
    "        ax2.set_ylabel('Accuracy (%)')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()     \n",
    "    \n",
    "    def plot_decision_boundary(self, model, x_train, y_train):\n",
    "        # Create grid first time only\n",
    "        if self.grid is None:\n",
    "            # Extend bounds a bit further for better visualization\n",
    "            x_min, x_max = x_train[:, 0].min() - 1.0, x_train[:, 0].max() + 1.0\n",
    "            y_min, y_max = x_train[:, 1].min() - 1.0, x_train[:, 1].max() + 1.0\n",
    "        \n",
    "            # Increase grid resolution for smoother boundaries\n",
    "            xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n",
    "                            np.linspace(y_min, y_max, 200))\n",
    "            self.grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "            self.grid_coords = (xx, yy)\n",
    "    \n",
    "        # Get predictions for grid points\n",
    "        grid_predictions = model.forward(self.grid)\n",
    "        grid_predictions = np.argmax(grid_predictions, axis=1)\n",
    "    \n",
    "        # Create new figure with white background\n",
    "        plt.figure(figsize=(10, 8), facecolor='white')\n",
    "    \n",
    "        # Plot decision boundary with better aesthetics\n",
    "        plt.contourf(self.grid_coords[0], self.grid_coords[1], \n",
    "                 grid_predictions.reshape(self.grid_coords[0].shape),\n",
    "                 alpha=0.15, cmap='viridis', levels=np.arange(4)-0.5)\n",
    "    \n",
    "        # Add contour lines to highlight boundaries\n",
    "        plt.contour(self.grid_coords[0], self.grid_coords[1],\n",
    "                grid_predictions.reshape(self.grid_coords[0].shape),\n",
    "                colors='black', alpha=0.3, linewidths=0.5)\n",
    "    \n",
    "        # Plot training points with better visibility\n",
    "        scatter = plt.scatter(x_train[:, 0], x_train[:, 1], \n",
    "                         c=np.argmax(y_train, axis=1), \n",
    "                         cmap='viridis',\n",
    "                         edgecolors='white',\n",
    "                         s=20,\n",
    "                         alpha=0.6,  # Some transparency\n",
    "                         linewidth=0.5)\n",
    "    \n",
    "        plt.colorbar(scatter, label='Class')\n",
    "        plt.xlabel('X', fontsize=12)\n",
    "        plt.ylabel('Y', fontsize=12)\n",
    "        plt.title('Decision Boundary', fontsize=14, pad=10)\n",
    "    \n",
    "        # Make plot more aesthetic\n",
    "        plt.grid(True, alpha=0.2)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def weights_gradients_heatmap(self, model: Sequential, optimizer: Optimizer) -> None:\n",
    "        '''\n",
    "        Plot the weights and their updates during training.\n",
    "        Args:\n",
    "            model: Sequential model to visualize\n",
    "            optimizer: Optimizer instance to calculate updates\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        # Get only hidden Linear layers\n",
    "        hidden_linear_layers = [(i, layer) for i, layer in enumerate(model.layers[1:-1]) \n",
    "                          if isinstance(layer, Linear)]\n",
    "    \n",
    "        if not hidden_linear_layers:\n",
    "            print(\"No hidden linear layers to visualize.\")\n",
    "            return\n",
    "    \n",
    "        # Create figure with 2 columns instead of 3 (combining gradients and updates)\n",
    "        fig, axes = plt.subplots(len(hidden_linear_layers), 2, \n",
    "                            figsize=(12, 4 * len(hidden_linear_layers)))\n",
    "    \n",
    "        # Handle single layer case\n",
    "        if len(hidden_linear_layers) == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "    \n",
    "        for i, (layer_num, layer) in enumerate(hidden_linear_layers):\n",
    "            # 1. Plot normalized weights\n",
    "            weights_norm = layer.weights / np.abs(layer.weights).max()\n",
    "            ax_weights = axes[i, 0]\n",
    "            cax_weights = ax_weights.matshow(weights_norm, cmap='RdBu', vmin=-1, vmax=1)\n",
    "            ax_weights.set_title(f'Layer {layer_num} Weights\\nMax absolute value: {np.abs(layer.weights).max():.4f}')\n",
    "            plt.colorbar(cax_weights, ax=ax_weights)\n",
    "        \n",
    "            # 2. Plot gradient-based updates\n",
    "            if layer.grad_weights is not None:\n",
    "                update = optimizer.learning_rate * layer.grad_weights\n",
    "                update_norm = update / np.abs(update).max()\n",
    "                ax_update = axes[i, 1]\n",
    "                cax_update = ax_update.matshow(update_norm, cmap='RdBu', vmin=-1, vmax=1)\n",
    "                ax_update.set_title(f'Layer {layer_num} Weight Updates (lr={optimizer.learning_rate})\\nMax absolute value: {np.abs(update).max():.4f}')\n",
    "                plt.colorbar(cax_update, ax=ax_update)\n",
    "    \n",
    "        plt.suptitle('Weight Values and Their Updates', y=1.05, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer that governs the training process, using the DataLoader, ValidationStrategy, and Optimizer\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model: Sequential, loss_fn: Loss, optimizer: Optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.visualizer = TrainingVisualizer()\n",
    "\n",
    "    def train(self, dataset: Dataset, epochs: int = 1000, log_interval:int = 200, show_plots:bool = True):\n",
    "        '''\n",
    "        Train the model using the specified loss function and optimizer.\n",
    "        The training loop consists of the following steps:\n",
    "        1. Split the dataset into training and validation sets.\n",
    "        2. For each epoch:\n",
    "            a. Iterate over the training set in batches.\n",
    "            b. Forward pass: Compute the predicted labels using the model.\n",
    "            c. Compute the loss using the loss function.\n",
    "            d. Backward pass: Compute the gradients of the loss with respect to the model parameters.\n",
    "            e. Update the model parameters using the optimizer.\n",
    "            f. Print the loss every 100 epochs.\n",
    "        3. Validate the model using the validation set.\n",
    "        4. Return the trained model.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to be used for training.\n",
    "            epochs (int): Number of epochs for training. Default is 1000.\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        \n",
    "        # Split the dataset into training and validation sets\n",
    "        train_loader, val_loader = DataLoader.holdout_split(dataset,test_size=0.2)\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                # Forward pass\n",
    "                y_pred = self.model.forward(x_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, probs = self.loss_fn.forward(y_batch, y_pred)\n",
    "                epoch_loss += loss*x_batch.shape[0]\n",
    "                # Backward pass\n",
    "                grad = self.loss_fn.backward(y_batch, probs)\n",
    "                self.model.backward(grad)\n",
    "                \n",
    "                # Update parameters\n",
    "                for layer in self.model.layers:\n",
    "                    if isinstance(layer, Linear):\n",
    "                        self.optimizer.step(layer.weights, layer.grad_weights)\n",
    "                        self.optimizer.step(layer.bias, layer.grad_bias)\n",
    "\n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            # Validate the model\n",
    "            val_loss = self.validate(val_loader)\n",
    "            self.visualizer.update(epoch_loss, val_loss, self.compute_accuracy(train_loader), self.compute_accuracy(val_loader))\n",
    "            # Print loss every 100 epochs\n",
    "            if epoch % log_interval == 0:\n",
    "                train_acc = self.compute_accuracy(train_loader)\n",
    "                val_acc = self.compute_accuracy(val_loader)\n",
    "                print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                f\"Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
    "                if show_plots:\n",
    "                    self.visualizer.plot_decision_boundary(self.model, train_loader.dataset.x, train_loader.dataset.y)\n",
    "                    self.visualizer.weights_gradients_heatmap(self.model, self.optimizer)\n",
    "        \n",
    "        # Plot the metrics history during training\n",
    "        self.visualizer.plot_metrics_history()\n",
    "                \n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        '''\n",
    "        Validate the model using the validation set.\n",
    "        The validation loop consists of the following steps:\n",
    "        1. Iterate over the validation set in batches.\n",
    "        2. Forward pass: Compute the predicted labels using the model.\n",
    "        3. Compute the loss using the loss function.\n",
    "        4. Return the average loss for the validation set.\n",
    "        Args:\n",
    "            val_loader (DataLoader): The DataLoader for the validation set.\n",
    "        Returns:\n",
    "            float: The average loss for the validation set.\n",
    "        '''\n",
    "        \n",
    "        val_loss = 0\n",
    "        for x_val, y_val in val_loader:\n",
    "            y_val_pred = self.model.forward(x_val)\n",
    "            val_loss += self.loss_fn.forward(y_val, y_val_pred)[0]\n",
    "        val_loss /= len(val_loader)\n",
    "        return val_loss\n",
    "    \n",
    "    def compute_accuracy(self, loader: DataLoader) -> float:\n",
    "        '''\n",
    "        Compute the accuracy of the model on the given DataLoader.\n",
    "        The accuracy is defined as the number of correct predictions divided by the total number of predictions.\n",
    "        Args:\n",
    "            loader (DataLoader): The DataLoader for the dataset.\n",
    "        Returns:\n",
    "            float: The accuracy of the model on the dataset.\n",
    "        ''' \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for x_batch, y_batch in loader:\n",
    "            pred = self.model.forward(x_batch)\n",
    "            correct += np.sum(np.argmax(pred, axis=1) == np.argmax(y_batch, axis=1))\n",
    "            total += len(y_batch)\n",
    "        return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44698a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossValidator class to perform k-fold cross-validation and other validation strategies\n",
    "#To be implemented\n",
    "\n",
    "class CrossValidator:\n",
    "    def __init__(self, model: Sequential, loss_fn: Loss, optimizer: Optimizer, validation_strategy : str = \"holdout\"):\n",
    "        assert validation_strategy in [\"holdout\", \"k-fold\"], \"Validation strategy must be either 'holdout' or 'k-fold'.\"\n",
    "        self.validation_strategy = validation_strategy\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer \n",
    "\n",
    "    def cross_validate(self, dataset: Dataset, epochs: int = 1000):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting everything together\n",
    "x_train, y_train = generate_spiral_data(1000, 3)\n",
    "x_test, y_test = generate_spiral_data(200, 3)\n",
    "dataset = Dataset(x_train, y_train)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(2, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 100),\n",
    "    ReLU(),\n",
    "    Linear(100, 3),\n",
    "])\n",
    "model.summary()\n",
    "loss_fn = CrossEntropySoftMax()\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "trainer = Trainer(model, loss_fn, optimizer)\n",
    "trainer.train(dataset, epochs=1000)\n",
    "# Test the model\n",
    "y_pred = model.forward(x_test)\n",
    "predicted_labels = np.argmax(y_pred, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
