{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0738778f",
   "metadata": {},
   "source": [
    "# From Scratch: Building a Neural Network with NumPy\n",
    "\n",
    "This notebook contains an end-to-end implementation of a simple feedforward neural network using **only NumPy** â€” no deep learning frameworks involved. It aims to demystify the inner workings of neural networks by walking through each component step-by-step, with a strong focus on **clarity, interactivity, and visualization**.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Manual forward and backward passes** (no autograd)\n",
    "- Training on toy datasets (e.g., digit recognition or synthetic classification)\n",
    "- Loss and accuracy plots updated in real-time\n",
    "- Visual explanation of gradients and weight updates\n",
    "- Interactive sliders or inputs (if supported) to adjust hyperparameters\n",
    "- Clean, well-commented code intended for learning and experimentation\n",
    "\n",
    "> *This project is inspired by university coursework, but developed independently from scratch to reinforce my understanding and extend the ideas further.*\n",
    "\n",
    "In case of any questions or comments, please contact me at ea.arseneva@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "970a1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e89cde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define abstract classes for Layer, Loss, Optimizer\n",
    "\n",
    "class Layer(ABC):\n",
    "    \n",
    "    def __init__(self, input_dim=None, output_dim=None):\n",
    "        self._input_dim = input_dim\n",
    "        self._output_dim = output_dim\n",
    "        self.input = None\n",
    "\n",
    "    @property\n",
    "    def input_dim(self):\n",
    "        return self._input_dim\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self._output_dim\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass through the layer.\n",
    "        Args:\n",
    "            x (np.ndarray): Input data. The shape should be (batch_size, input_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Output data. The shape should be (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the layer.\n",
    "        Args:\n",
    "            grad (np.ndarray): Gradient of the loss with respect to the output. \n",
    "            The shape should be (batch_size, output_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the input.\n",
    "            The shape should be (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Forward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            float: Computed loss. The shape should be a scalar.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the predictions. The shape should be (batch_size,).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class Optimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Update the parameters based on the gradients.\n",
    "        Args:\n",
    "            params (np.ndarray): Parameters to be updated. The shape should be (num_params,).\n",
    "            grads (np.ndarray): Gradients of the loss with respect to the parameters. The shape should be (num_params,).\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Layer: Linear, ReLU, and Sequential\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.input_dim, f\"Input shape {x.shape} does not match expected shape (batch_size, {self.input_dim})\"\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape == self.input.shape, f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of ReLU is 1 for positive inputs, 0 for negative inputs\n",
    "        return grad * (self.input > 0)\n",
    "    \n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        assert input_dim > 0 and output_dim > 0, \"Input and output dimensions of a Linear layer must be positive integers.\"\n",
    "        super().__init__(input_dim, output_dim)\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.weights.shape[0], f\"Input shape {x.shape} does not match expected shape (batch_size, {self.weights.shape[0]})\"\n",
    "        assert self.weights.shape[1] == self.bias.shape[1], f\"Weights shape {self.weights.shape} does not match bias shape {self.bias.shape}\"\n",
    "        assert x.shape[0] == self.bias.shape[0], f\"Input shape {x.shape} does not match bias shape {self.bias.shape}\"\n",
    "        self.input = x\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape[1] == self.bias.shape[1], f\"Gradient shape {grad.shape} does not match bias shape {self.bias.shape}\"\n",
    "        assert grad.shape[0] == self.input.shape[0], f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of the loss with respect to the input\n",
    "        grad_input = grad @ self.weights.T\n",
    "        # Gradient of the loss with respect to the weights and bias\n",
    "        grad_weights = self.input.T @ grad\n",
    "        grad_bias = np.sum(grad, axis=0, keepdims=True)\n",
    "        # Update weights and bias here if needed\n",
    "        return grad_input\n",
    "    \n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        super().__init__(layers[0].input_dim, layers[-1].output_dim)\n",
    "        self.__check_consistency__()\n",
    "\n",
    "    def __check_consistency__(self):\n",
    "        assert len(self.layers) > 0, \"Sequential model must have at least one layer.\"\n",
    "        assert self.layers[0].input_dim is not None, \"First layer input dimension must be specified.\"\n",
    "        assert self.layers[-1].output_dim is not None, \"Last layer output dimension must be specified.\"\n",
    "        assert self.layers[0].input_dim == self.input_dim, f\"First layer input dimension {self.layers[0].input_dim} does not match expected input dimension {self.input_dim}\"\n",
    "        assert self.layers[-1].output_dim == self.output_dim, f\"Last layer output dimension {self.layers[-1].output_dim} does not match expected output dimension {self.output_dim}\"\n",
    "        current_dim = self.input_dim\n",
    "        mismatch_list = []\n",
    "        for layer in self.layers:\n",
    "            if layer.input_dim != None:\n",
    "                if layer.input_dim != current_dim: \n",
    "                    mismatch_list.append(f\"Layer {layer.__class__.__name__} input dimension {layer.input_dim} does not match expected input dimension {current_dim}\")\n",
    "                current_dim = layer.output_dim\n",
    "        if current_dim != self.layers[-1].input_dim: \n",
    "            mismatch_list.append(f\"Last layer input dimension {self.layers[-1].input_dim} does not match expected input dimension {current_dim}\")\n",
    "        assert len(mismatch_list) == 0, f\"Layer dimension mismatch: {'\\n'.join(mismatch_list)}\"\n",
    "            \n",
    "             \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.layers[0].input_dim, f\"Input shape {x.shape} does not match expected shape (batch_size, {self.layers[0].input_dim})\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Loss: MeanSquaredError and CrossEntropy\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "    \n",
    "    \n",
    "class CrossEntropy(Loss):\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean((y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)), axis=1)\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        # Clip predictions to prevent division by zero\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Optimizer: SGD\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.01):\n",
    "        assert learning_rate > 0, \"Learning rate must be a positive number.\"\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        assert params.shape == grads.shape, f\"Parameters shape {params.shape} does not match gradients shape {grads.shape}\"\n",
    "        params -= self.learning_rate * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26f1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90a693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
