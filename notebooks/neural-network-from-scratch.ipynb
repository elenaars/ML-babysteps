{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0738778f",
   "metadata": {},
   "source": [
    "# From Scratch: Building a Neural Network with NumPy\n",
    "\n",
    "This notebook contains an end-to-end implementation of a simple feedforward neural network using **only NumPy** â€” no deep learning frameworks involved. It aims to demystify the inner workings of neural networks by walking through each component step-by-step, with a strong focus on **clarity, interactivity, and visualization**.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Manual forward and backward passes** (no autograd)\n",
    "- Training on toy datasets (e.g., digit recognition or synthetic classification)\n",
    "- Loss and accuracy plots updated in real-time\n",
    "- Visual explanation of gradients and weight updates\n",
    "- Interactive sliders or inputs (if supported) to adjust hyperparameters\n",
    "- Clean, well-commented code intended for learning and experimentation\n",
    "\n",
    "> *This project is inspired by university coursework, but developed independently from scratch to reinforce my understanding and extend the ideas further.*\n",
    "\n",
    "In case of any questions or comments, please contact me at ea.arseneva@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    # Subtract max for numerical stability\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define abstract classes for Layer, Loss, Optimizer\n",
    "\n",
    "class Layer(ABC):\n",
    "    \n",
    "    def __init__(self, input_dim=None, output_dim=None):\n",
    "        self._input_dim = input_dim\n",
    "        self._output_dim = output_dim\n",
    "        self.input = None\n",
    "\n",
    "    @property\n",
    "    def input_dim(self):\n",
    "        return self._input_dim\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self._output_dim\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass through the layer.\n",
    "        Args:\n",
    "            x (np.ndarray): Input data. The shape should be (batch_size, input_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Output data. The shape should be (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the layer.\n",
    "        Args:\n",
    "            grad (np.ndarray): Gradient of the loss with respect to the output. \n",
    "            The shape should be (batch_size, output_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the input.\n",
    "            The shape should be (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Forward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            float: Computed loss. The shape should be a scalar.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the predictions. The shape should be (batch_size,).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class Optimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Update the parameters based on the gradients.\n",
    "        Args:\n",
    "            params (np.ndarray): Parameters to be updated. The shape should be (num_params,).\n",
    "            grads (np.ndarray): Gradients of the loss with respect to the parameters. The shape should be (num_params,).\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Layer: Linear, ReLU, SoftMax, and Sequential\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape == self.input.shape, f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of ReLU is 1 for positive inputs, 0 for negative inputs\n",
    "        return grad * (self.input > 0)\n",
    "    \n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        assert input_dim > 0 and output_dim > 0, \"Input and output dimensions of a Linear layer must be positive integers.\"\n",
    "        super().__init__(input_dim, output_dim)\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None   \n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.weights.shape[0], f\"Input shape {x.shape} does not match expected shape (batch_size, {self.weights.shape[0]})\"\n",
    "        assert self.weights.shape[1] == self.bias.shape[1], f\"Weights shape {self.weights.shape} does not match bias shape {self.bias.shape}\"\n",
    "        self.input = x\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape[1] == self.bias.shape[1], f\"Gradient shape {grad.shape} does not match bias shape {self.bias.shape}\"\n",
    "        assert grad.shape[0] == self.input.shape[0], f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of the loss with respect to the input\n",
    "        grad_input = grad @ self.weights.T\n",
    "        # Gradient of the loss with respect to the weights and bias\n",
    "        self.grad_weights = self.input.T @ grad\n",
    "        self.grad_bias = np.sum(grad, axis=0, keepdims=True)\n",
    "        return grad_input \n",
    "    \n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        super().__init__(layers[0].input_dim, layers[-1].output_dim)\n",
    "        self.__check_consistency__()\n",
    "\n",
    "    def __check_consistency__(self):\n",
    "        assert len(self.layers) > 1, \"Sequential model must have at least one layer.\"\n",
    "        assert self.layers[0].input_dim is not None, \"First layer input dimension must be specified.\"\n",
    "        # here we assume the last layer is a  softmax layer, so we check the second last layer\n",
    "        assert self.layers[-1].output_dim is not None, \"Last layer output dimension must be specified.\"\n",
    "        assert self.layers[0].input_dim == self.input_dim, f\"First layer input dimension {self.layers[0].input_dim} does not match expected input dimension {self.input_dim}\"\n",
    "        assert self.layers[-1].output_dim == self.output_dim, f\"Last layer output dimension {self.layers[-1].output_dim} does not match expected output dimension {self.output_dim}\"\n",
    "        current_dim = self.input_dim\n",
    "        mismatch_list = []\n",
    "        for layer in self.layers:\n",
    "            if layer.input_dim != None:\n",
    "                if layer.input_dim != current_dim: \n",
    "                    mismatch_list.append(f\"Layer {layer.__class__.__name__} input dimension {layer.input_dim} does not match expected input dimension {current_dim}\")\n",
    "                current_dim = layer.output_dim\n",
    "       # if current_dim != self.layers[-2].output_dim: \n",
    "       #     mismatch_list.append(f\"Last layer output dimension {self.layers[-2].output_dim} does not match expected output dimension {current_dim}\")\n",
    "        assert len(mismatch_list) == 0, f\"Layer dimension mismatch: {'\\n'.join(mismatch_list)}\"\n",
    "            \n",
    "             \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.layers[0].input_dim, f\"Input shape {x.shape} does not match expected shape (batch_size, {self.layers[0].input_dim})\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Loss: MeanSquaredError and CrossEntropy\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> tuple:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return np.mean(np.square(y_true - y_pred)), y_pred\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "    \n",
    "    \n",
    "class CrossEntropySoftMax(Loss):\n",
    "    # This loss function is used for multi-class classification problems.\n",
    "    # It combines softmax activation and cross-entropy loss in one function.\n",
    "    \n",
    "    def forward(self, y_true: np.ndarray, y_pred_logits: np.ndarray) -> tuple:\n",
    "        assert y_true.shape == y_pred_logits.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred_logits.shape}\"\n",
    "        #apply softmax to the predictions\n",
    "        probs = softmax(y_pred_logits)\n",
    "        loss = -np.sum(y_true * np.log(probs + 1e-15)) / y_true.shape[0]\n",
    "        return loss, probs\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray, probs: np.ndarray) -> np.ndarray:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        # Clip predictions to prevent division by zero\n",
    "        return (probs - y_true)/y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Optimizer: SGD\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.01):\n",
    "        assert learning_rate > 0, \"Learning rate must be a positive number.\"\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        assert params.shape == grads.shape, f\"Parameters shape {params.shape} does not match gradients shape {grads.shape}\"\n",
    "        params -= self.learning_rate * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything together in a training loop\n",
    "def train(model: Sequential, loss_fn: Loss, optimizer: Optimizer, x_train: np.ndarray, y_train: np.ndarray, batch_size: int = 32, epochs: int = 1000):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        # go in batches\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model.forward(x_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            batch_loss, probs = loss_fn.forward(y_batch, y_pred)\n",
    "            epoch_loss += batch_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            grad = loss_fn.backward(y_batch, y_pred, probs)\n",
    "            model.backward(grad)\n",
    "            \n",
    "            # Update parameters\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, Linear):\n",
    "                    optimizer.step(layer.weights, layer.grad_weights)\n",
    "                    optimizer.step(layer.bias, layer.grad_bias)\n",
    "        # Average loss for the epoch\n",
    "        epoch_loss /= (x_train.shape[0] // batch_size)\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        #if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a simple model and apply it to spiral dataset\n",
    "def generate_spiral_data(n_points_per_class: int, n_classes: int):\n",
    "    x = []\n",
    "    y = []\n",
    "    for j in range(n_classes):\n",
    "        ix = range(n_points_per_class * j, n_points_per_class * (j + 1))\n",
    "        r = np.linspace(0.0, 1, n_points_per_class)\n",
    "        t = np.linspace(j * 4, (j + 1) * 4, n_points_per_class) + np.random.randn(n_points_per_class) * 0.2\n",
    "        x1 = r * np.sin(t)\n",
    "        x2 = r * np.cos(t)\n",
    "        x.append(np.c_[x1, x2])\n",
    "        y.append(np.full(n_points_per_class, j))\n",
    "    x = np.vstack(x)\n",
    "    y = np.hstack(y)\n",
    "    y_one_hot = np.eye(n_classes)[y]\n",
    "    return x, y_one_hot\n",
    "\n",
    "\n",
    "x_train, y_train = generate_spiral_data(100, 3)\n",
    "x_test, y_test = generate_spiral_data(20, 3)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(2, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 3),\n",
    "])\n",
    "loss_fn = CrossEntropySoftMax()\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "#batch_size = 32\n",
    "# Train the model\n",
    "train(model, loss_fn, optimizer, x_train, y_train, epochs=1000, batch_size=10)\n",
    "# Test the model\n",
    "y_pred = model.forward(x_test)\n",
    "predicted_labels = np.argmax(y_pred, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758906f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader classes to wrap the data and operate on batches\n",
    "class Dataset:\n",
    "    def __init__(self, x: np.ndarray, y: np.ndarray):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert isinstance(index, (int, np.ndarray)), \"Index must be an integer or a numpy array.\"\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "class DataLoader:\n",
    "    def __init__(self, dataset: Dataset, indices = None, batch_size: int = 32, shuffle: bool = False):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = indices if indices is not None else np.arange(len(dataset))\n",
    "        self.current_index = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_index >= len(self.indices):\n",
    "            raise StopIteration\n",
    "        start_index = self.current_index\n",
    "        end_index = min(start_index + self.batch_size, len(self.indices))\n",
    "        batch_indices = self.indices[start_index:end_index]\n",
    "        x_batch, y_batch = self.dataset[batch_indices]\n",
    "        self.current_index += self.batch_size\n",
    "        return x_batch, y_batch\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.indices) / self.batch_size))\n",
    "    \n",
    "    @staticmethod\n",
    "    def holdout_split(dataset: Dataset, test_size: float = 0.2):\n",
    "        \"\"\"\n",
    "        Splits the dataset into training and testing sets.\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset to split.\n",
    "            test_size (float): The proportion of the dataset to include in the test split.\n",
    "        Returns:\n",
    "            DataLoader: Loader for the training portion of the dataset.\n",
    "            DataLoader: Loader for the testing portion of the dataset.\n",
    "        \"\"\"\n",
    "        assert 0 < test_size < 1, \"test_size must be between 0 and 1.\"\n",
    "        indices = np.arange(len(dataset))\n",
    "        np.random.shuffle(indices)\n",
    "        split_index = int(len(dataset) * (1 - test_size))\n",
    "        train_indices = indices[:split_index]\n",
    "        test_indices = indices[split_index:]\n",
    "        return DataLoader(dataset, train_indices), DataLoader(dataset, test_indices)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e0e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer that governs the training process, using the DataLoader, ValidationStrategy, and Optimizer\n",
    "class Trainer:\n",
    "    def __init__(self, model: Sequential, loss_fn: Loss, optimizer: Optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def train(self, dataset: Dataset, epochs: int = 1000):\n",
    "        # Split the dataset into training and validation sets\n",
    "        train_loader, val_loader = DataLoader.holdout_split(dataset,test_size=0.2)\n",
    "        # Training loop\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                # Forward pass\n",
    "                y_pred = self.model.forward(x_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss, probs = self.loss_fn.forward(y_batch, y_pred)\n",
    "                epoch_loss += loss*x_batch.shape[0]\n",
    "                # Backward pass\n",
    "                grad = self.loss_fn.backward(y_batch, y_pred, probs)\n",
    "                self.model.backward(grad)\n",
    "                \n",
    "                # Update parameters\n",
    "                for layer in self.model.layers:\n",
    "                    if isinstance(layer, Linear):\n",
    "                        self.optimizer.step(layer.weights, layer.grad_weights)\n",
    "                        self.optimizer.step(layer.bias, layer.grad_bias)\n",
    "\n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "            # Validate the model\n",
    "            val_loss = self.validate(val_loader)\n",
    "            print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "            \n",
    "    def validate(self, val_loader: DataLoader) -> float:\n",
    "        val_loss = 0\n",
    "        #count = 0\n",
    "        for x_val, y_val in val_loader:\n",
    "            y_val_pred = self.model.forward(x_val)\n",
    "            val_loss += self.loss_fn.forward(y_val, y_val_pred)[0]\n",
    "           # count += 1\n",
    "        val_loss /= len(val_loader)\n",
    "        return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44698a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossValidator class to perform k-fold cross-validation and other validation strategies\n",
    "\n",
    "class CrossValidator:\n",
    "    def __init__(self, model: Sequential, loss_fn: Loss, optimizer: Optimizer, validation_strategy : str = \"holdout\"):\n",
    "        assert validation_strategy in [\"holdout\", \"k-fold\"], \"Validation strategy must be either 'holdout' or 'k-fold'.\"\n",
    "        self.validation_strategy = validation_strategy\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.validation_strategy = validation_strategy\n",
    "\n",
    "    def cross_validate(self, dataset: Dataset, epochs: int = 1000):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c2fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting everything together\n",
    "x_train, y_train = generate_spiral_data(1000, 3)\n",
    "x_test, y_test = generate_spiral_data(200, 3)\n",
    "dataset = Dataset(x_train, y_train)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(2, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 3),\n",
    "])\n",
    "loss_function = CrossEntropySoftMax()\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "trainer = Trainer(model, loss_function, optimizer)\n",
    "trainer.train(dataset, epochs=1000)\n",
    "# Test the model\n",
    "y_pred = model.forward(x_test)\n",
    "predicted_labels = np.argmax(y_pred, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
