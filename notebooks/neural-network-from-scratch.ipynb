{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0738778f",
   "metadata": {},
   "source": [
    "# From Scratch: Building a Neural Network with NumPy\n",
    "\n",
    "This notebook contains an end-to-end implementation of a simple feedforward neural network using **only NumPy** â€” no deep learning frameworks involved. It aims to demystify the inner workings of neural networks by walking through each component step-by-step, with a strong focus on **clarity, interactivity, and visualization**.\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Manual forward and backward passes** (no autograd)\n",
    "- Training on toy datasets (e.g., digit recognition or synthetic classification)\n",
    "- Loss and accuracy plots updated in real-time\n",
    "- Visual explanation of gradients and weight updates\n",
    "- Interactive sliders or inputs (if supported) to adjust hyperparameters\n",
    "- Clean, well-commented code intended for learning and experimentation\n",
    "\n",
    "> *This project is inspired by university coursework, but developed independently from scratch to reinforce my understanding and extend the ideas further.*\n",
    "\n",
    "In case of any questions or comments, please contact me at ea.arseneva@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970a1e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89cde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define abstract classes for Layer, Loss, Optimizer\n",
    "\n",
    "class Layer(ABC):\n",
    "    \n",
    "    def __init__(self, input_dim=None, output_dim=None):\n",
    "        self._input_dim = input_dim\n",
    "        self._output_dim = output_dim\n",
    "        self.input = None\n",
    "\n",
    "    @property\n",
    "    def input_dim(self):\n",
    "        return self._input_dim\n",
    "\n",
    "    @property\n",
    "    def output_dim(self):\n",
    "        return self._output_dim\n",
    "    \n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Forward pass through the layer.\n",
    "        Args:\n",
    "            x (np.ndarray): Input data. The shape should be (batch_size, input_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Output data. The shape should be (batch_size, output_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the layer.\n",
    "        Args:\n",
    "            grad (np.ndarray): Gradient of the loss with respect to the output. \n",
    "            The shape should be (batch_size, output_dim).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the input.\n",
    "            The shape should be (batch_size, input_dim).\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class Loss(ABC):\n",
    "    @abstractmethod\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Forward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            float: Computed loss. The shape should be a scalar.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Backward pass through the loss function.\n",
    "        Args:\n",
    "            y_true (np.ndarray): True labels. The shape should be (batch_size,).\n",
    "            y_pred (np.ndarray): Predicted labels. The shape should be (batch_size,).\n",
    "        Returns:\n",
    "            np.ndarray: Gradient of the loss with respect to the predictions. The shape should be (batch_size,).\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "class Optimizer(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Update the parameters based on the gradients.\n",
    "        Args:\n",
    "            params (np.ndarray): Parameters to be updated. The shape should be (num_params,).\n",
    "            grads (np.ndarray): Gradients of the loss with respect to the parameters. The shape should be (num_params,).\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7162cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Layer: Linear, ReLU, SoftMax, and Sequential\n",
    "\n",
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        self.input = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape == self.input.shape, f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of ReLU is 1 for positive inputs, 0 for negative inputs\n",
    "        return grad * (self.input > 0)\n",
    "    \n",
    "    \n",
    "class Linear(Layer):\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        assert input_dim > 0 and output_dim > 0, \"Input and output dimensions of a Linear layer must be positive integers.\"\n",
    "        super().__init__(input_dim, output_dim)\n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.zeros((1, output_dim))\n",
    "        self.grad_weights = None\n",
    "        self.grad_bias = None   \n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.weights.shape[0], f\"Input shape {x.shape} does not match expected shape (batch_size, {self.weights.shape[0]})\"\n",
    "        assert self.weights.shape[1] == self.bias.shape[1], f\"Weights shape {self.weights.shape} does not match bias shape {self.bias.shape}\"\n",
    "        self.input = x\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape[1] == self.bias.shape[1], f\"Gradient shape {grad.shape} does not match bias shape {self.bias.shape}\"\n",
    "        assert grad.shape[0] == self.input.shape[0], f\"Gradient shape {grad.shape} does not match input shape {self.input.shape}\"\n",
    "        # Gradient of the loss with respect to the input\n",
    "        grad_input = grad @ self.weights.T\n",
    "        # Gradient of the loss with respect to the weights and bias\n",
    "        self.grad_weights = self.input.T @ grad\n",
    "        self.grad_bias = np.sum(grad, axis=0, keepdims=True)\n",
    "        return grad_input\n",
    "    \n",
    "class SoftMax(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        # Subtract max for numerical stability\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        assert grad.shape == self.output.shape, f\"Gradient shape {grad.shape} does not match output shape {self.output.shape}\"\n",
    "        # Gradient of the loss with respect to the input\n",
    "        return grad * self.output * (1 - self.output)\n",
    "    \n",
    "class Sequential(Layer):\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers = layers\n",
    "        super().__init__(layers[0].input_dim, layers[-2].output_dim)\n",
    "        self.__check_consistency__()\n",
    "\n",
    "    def __check_consistency__(self):\n",
    "        assert len(self.layers) > 1, \"Sequential model must have at least one layer.\"\n",
    "        assert self.layers[0].input_dim is not None, \"First layer input dimension must be specified.\"\n",
    "        # here we assume the last layer is a  softmax layer, so we check the second last layer\n",
    "        assert self.layers[-2].output_dim is not None, \"Last layer output dimension must be specified.\"\n",
    "        assert self.layers[0].input_dim == self.input_dim, f\"First layer input dimension {self.layers[0].input_dim} does not match expected input dimension {self.input_dim}\"\n",
    "        assert self.layers[-2].output_dim == self.output_dim, f\"Last layer output dimension {self.layers[-1].output_dim} does not match expected output dimension {self.output_dim}\"\n",
    "        current_dim = self.input_dim\n",
    "        mismatch_list = []\n",
    "        for layer in self.layers:\n",
    "            if layer.input_dim != None:\n",
    "                if layer.input_dim != current_dim: \n",
    "                    mismatch_list.append(f\"Layer {layer.__class__.__name__} input dimension {layer.input_dim} does not match expected input dimension {current_dim}\")\n",
    "                current_dim = layer.output_dim\n",
    "       # if current_dim != self.layers[-2].output_dim: \n",
    "       #     mismatch_list.append(f\"Last layer output dimension {self.layers[-2].output_dim} does not match expected output dimension {current_dim}\")\n",
    "        assert len(mismatch_list) == 0, f\"Layer dimension mismatch: {'\\n'.join(mismatch_list)}\"\n",
    "            \n",
    "             \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert x.shape[1] == self.layers[0].input_dim, f\"Input shape {x.shape} does not match expected shape (batch_size, {self.layers[0].input_dim})\"\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb14196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Loss: MeanSquaredError and CrossEntropy\n",
    "\n",
    "class MeanSquaredError(Loss):\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n",
    "    \n",
    "    \n",
    "class CrossEntropy(Loss):\n",
    "    def forward(self, y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean((y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)), axis=1)\n",
    "\n",
    "    def backward(self, y_true: np.ndarray, y_pred: np.ndarray) -> np.ndarray:\n",
    "        assert y_true.shape == y_pred.shape, f\"True labels shape {y_true.shape} does not match predicted labels shape {y_pred.shape}\"\n",
    "        # Clip predictions to prevent division by zero\n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cca352b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define concrete implementation of Optimizer: SGD\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    def __init__(self, learning_rate: float = 0.01):\n",
    "        assert learning_rate > 0, \"Learning rate must be a positive number.\"\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def step(self, params: np.ndarray, grads: np.ndarray) -> None:\n",
    "        assert params.shape == grads.shape, f\"Parameters shape {params.shape} does not match gradients shape {grads.shape}\"\n",
    "        params -= self.learning_rate * grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26f1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put everything together in a training loop\n",
    "def train(model: Sequential, loss_fn: Loss, optimizer: Optimizer, x_train: np.ndarray, y_train: np.ndarray, batch_size: int = 32, epochs: int = 1000):\n",
    "    for epoch in range(epochs):\n",
    "        # go in batches\n",
    "        for i in range(0, x_train.shape[0], batch_size):\n",
    "            x_batch = x_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_pred = model.forward(x_batch)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn.forward(y_batch, y_pred)\n",
    "            \n",
    "            # Backward pass\n",
    "            grad = loss_fn.backward(y_batch, y_pred)\n",
    "            model.backward(grad)\n",
    "            \n",
    "            # Update parameters\n",
    "            for layer in model.layers:\n",
    "                if isinstance(layer, Linear):\n",
    "                    optimizer.step(layer.weights, layer.grad_weights)\n",
    "                    optimizer.step(layer.bias, layer.grad_bias)\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba90a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a simple model and apply it to spiral dataset\n",
    "def generate_spiral_data(n_points_per_class: int, n_classes: int):\n",
    "    x = []\n",
    "    y = []\n",
    "    for j in range(n_classes):\n",
    "        ix = range(n_points_per_class * j, n_points_per_class * (j + 1))\n",
    "        r = np.linspace(0.0, 1, n_points_per_class)\n",
    "        t = np.linspace(j * 4, (j + 1) * 4, n_points_per_class) + np.random.randn(n_points_per_class) * 0.2\n",
    "        x1 = r * np.sin(t)\n",
    "        x2 = r * np.cos(t)\n",
    "        x.append(np.c_[x1, x2])\n",
    "        y.append(np.full(n_points_per_class, j))\n",
    "    x = np.vstack(x)\n",
    "    y = np.hstack(y)\n",
    "    y_one_hot = np.eye(n_classes)[y]\n",
    "    return x, y_one_hot\n",
    "\n",
    "\n",
    "x_train, y_train = generate_spiral_data(1000, 3)\n",
    "x_test, y_test = generate_spiral_data(200, 3)\n",
    "\n",
    "model = Sequential([\n",
    "    Linear(2, 64),\n",
    "    ReLU(),\n",
    "    Linear(64, 3),\n",
    "    SoftMax()\n",
    "])\n",
    "loss_fn = CrossEntropy()\n",
    "optimizer = SGD(learning_rate=0.01)\n",
    "batch_size = 32\n",
    "# Train the model\n",
    "train(model, loss_fn, optimizer, x_train, y_train, epochs=1000)\n",
    "# Test the model\n",
    "y_pred = model.forward(x_test)\n",
    "predicted_labels = np.argmax(y_pred, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "accuracy = np.mean(predicted_labels == true_labels)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
